{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of principal actions (panel C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "from definitions import ROOT_DIR, NUM_MUSCLES\n",
    "from sklearn.decomposition import PCA\n",
    "from envs.environment_factory import EnvironmentFactory\n",
    "from main_eval import load_vecnormalize, load_model\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small variation datasets\n",
    "df_name_dict = {\n",
    "    \"baoding\": \"baoding_sv.h5\",\n",
    "    \"early_baoding\": \"baoding_step_12.h5\",\n",
    "    \"hand_pose\": \"hand_pose.h5\",\n",
    "    \"hand_reach\": \"hand_reach.h5\",\n",
    "    \"pen\": \"pen.h5\",\n",
    "    \"reorient\": \"reorient.h5\",\n",
    "\n",
    "}\n",
    "df_dict = {\n",
    "    key: pd.read_hdf(os.path.join(ROOT_DIR, \"data\", \"datasets\", value))\n",
    "    for key, value in df_name_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate the model when projecting the actions on a subset of the PCs\n",
    "# best_to_worst removes low variance components first, worst to best remove the \n",
    "# high variance components first\n",
    "n_comp = NUM_MUSCLES\n",
    "num_ep = 10\n",
    "save_results = False\n",
    "target_task_list = [\"pen\"]  # , \"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "sorting = \"best_to_worst\"  # \"best_to_worst\", \"worst_to_best\"\n",
    "env_config = {\n",
    "        \"env_name\": \"MyoBaodingBallsP1\",\n",
    "        \"weighted_reward_keys\": {\n",
    "            \"pos_dist_1\": 0,\n",
    "            \"pos_dist_2\": 0,\n",
    "            \"act_reg\": 0,\n",
    "            \"solved\": 5,\n",
    "            \"done\": 0,  \n",
    "            \"sparse\": 0,\n",
    "        },\n",
    "        # # \"goal_time_period\": [4, 6],  # phase 2: (4, 6)\n",
    "        # # \"goal_xrange\": (0.020, 0.030),  # phase 2: (0.020, 0.030)\n",
    "        # # \"goal_yrange\": (0.022, 0.032),  # phase 2: (0.022, 0.032)\n",
    "        # # # Randomization in physical properties of the baoding balls\n",
    "        \"obj_size_range\": (\n",
    "            0.020,\n",
    "            0.022,\n",
    "        ),  # (0.018, 0.024)   # Object size range. Nominal 0.022\n",
    "        \"obj_mass_range\": (\n",
    "            0.14,\n",
    "            0.16,\n",
    "            # # \"obj_friction_change\": (0.2, 0.001, 0.00002),  # (0.2, 0.001, 0.00002)\n",
    "        ),\n",
    "        \"task_choice\": \"fixed\",\n",
    "        \"seed\": 42\n",
    "    }\n",
    "env_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "model_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "for target_task in target_task_list:\n",
    "    # First choose the pca to use to project the actions\n",
    "    actions = np.vstack(df_dict[target_task].action)\n",
    "    pca = PCA(n_components=n_comp).fit(actions)\n",
    "\n",
    "    env = EnvironmentFactory.create(**env_config)\n",
    "    vecnormalize = load_vecnormalize(env_path, env)\n",
    "    vecnormalize.training = False\n",
    "    vecnormalize.norm_reward = False\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    performance = []\n",
    "    for k in range(n_comp):\n",
    "        print(\"Environment: \", target_task, \"component \", k)\n",
    "        if sorting == \"best_to_worst\":\n",
    "            components = pca.components_[:n_comp-k]\n",
    "        elif sorting == \"worst_to_best\":\n",
    "            components = pca.components_[k:]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sorting: \", sorting)\n",
    "\n",
    "        performance_ep = []\n",
    "        for n in range(num_ep):\n",
    "            acts = []\n",
    "            cum_reward = 0\n",
    "            lstm_states = None\n",
    "            obs = env.reset()\n",
    "            episode_starts = np.ones((1,), dtype=bool)\n",
    "            done = False\n",
    "            timestep = 0\n",
    "            while not done: \n",
    "                timestep += 1\n",
    "                action, lstm_states = model.predict(vecnormalize.normalize_obs(obs),\n",
    "                                                        state=lstm_states,\n",
    "                                                        episode_start=episode_starts,\n",
    "                                                        deterministic=True,\n",
    "                                                        )\n",
    "                \n",
    "                action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "                action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "                obs, rewards, done, info = env.step(action_backproj.reshape(39,))\n",
    "                episode_starts = done\n",
    "                cum_reward += rewards\n",
    "            performance_ep.append(cum_reward)\n",
    "            print(f\"Episode {n}, reward: {cum_reward}\")\n",
    "        performance_ep = np.array(performance_ep) / 1000  # Transform the reward into the solved fraction\n",
    "        data_point = {'components':components,'solved_frac_mean': np.mean(performance_ep), 'solved_frac_sem': np.std(performance_ep) / np.sqrt(len(performance_ep))}\n",
    "        performance.append(data_point)\n",
    "    if save_results:\n",
    "        fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_action_components_{target_task}_{sorting}.pkl\")\n",
    "        fp_acts_pcs = open(fp, 'wb')\n",
    "        pickle.dump(performance, fp_acts_pcs)\n",
    "        fp_acts_pcs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate step 12 on all the other PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate the model when projecting the actions on a subset of the PCs\n",
    "# best_to_worst removes low variance components first, worst to best remove the \n",
    "# high variance components first\n",
    "n_comp = NUM_MUSCLES\n",
    "num_ep = 10\n",
    "save_results = False\n",
    "target_task_list = [\"baoding\"]  #, \"early_baoding\", \"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "sorting = \"best_to_worst\"  # \"best_to_worst\", \"worst_to_best\"\n",
    "env_config = {\n",
    "        \"env_name\": \"CustomMyoBaodingBallsP1\",\n",
    "        \"weighted_reward_keys\": {\n",
    "            \"pos_dist_1\": 0,\n",
    "            \"pos_dist_2\": 0,\n",
    "            \"act_reg\": 0,\n",
    "            \"solved\": 5,\n",
    "            \"done\": 0,\n",
    "            \"sparse\": 0,\n",
    "        },\n",
    "        \"task_choice\": \"fixed\",\n",
    "        \"goal_time_period\": (5, 5),\n",
    "        \"seed\": 42\n",
    "    }\n",
    "env_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/12_period_5/env.pkl\",\n",
    ")\n",
    "model_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/12_period_5/model.zip\",\n",
    ")\n",
    "\n",
    "\n",
    "for target_task in target_task_list:\n",
    "    # First choose the pca to use to project the actions\n",
    "    actions = np.vstack(df_dict[target_task].action)\n",
    "    pca = PCA(n_components=n_comp).fit(actions)\n",
    "\n",
    "    env = EnvironmentFactory.create(**env_config)\n",
    "    vecnormalize = load_vecnormalize(env_path, env)\n",
    "    vecnormalize.training = False\n",
    "    vecnormalize.norm_reward = False\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    performance = []\n",
    "    for k in range(n_comp):\n",
    "        print(\"Environment: \", target_task, \"component \", k)\n",
    "        if sorting == \"best_to_worst\":\n",
    "            components = pca.components_[:n_comp-k]\n",
    "        elif sorting == \"worst_to_best\":\n",
    "            components = pca.components_[k:]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sorting: \", sorting)\n",
    "\n",
    "        performance_ep = []\n",
    "        for n in range(num_ep):\n",
    "            acts = []\n",
    "            cum_reward = 0\n",
    "            lstm_states = None\n",
    "            obs = env.reset(random_phase=0)\n",
    "            episode_starts = np.ones((1,), dtype=bool)\n",
    "            done = False\n",
    "            timestep = 0\n",
    "            while not done: \n",
    "                timestep += 1\n",
    "                action, lstm_states = model.predict(vecnormalize.normalize_obs(obs),\n",
    "                                                        state=lstm_states,\n",
    "                                                        episode_start=episode_starts,\n",
    "                                                        deterministic=True,\n",
    "                                                        )\n",
    "                \n",
    "                action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "                action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "                obs, rewards, done, info = env.step(action_backproj.reshape(39,))\n",
    "                episode_starts = done\n",
    "                cum_reward += rewards\n",
    "            performance_ep.append(cum_reward)\n",
    "            print(f\"Episode {n}, reward: {cum_reward}\")\n",
    "        performance_ep = np.array(performance_ep) / 1000  # Transform the reward into the solved fraction\n",
    "        data_point = {'components':components,'solved_frac_mean': np.mean(performance_ep), 'solved_frac_sem': np.std(performance_ep) / np.sqrt(len(performance_ep))}\n",
    "        performance.append(data_point)\n",
    "\n",
    "    if save_results:\n",
    "        fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_step_12_{target_task}_{sorting}.pkl\")\n",
    "        fp_acts_pcs = open(fp, 'wb')\n",
    "        pickle.dump(performance, fp_acts_pcs)\n",
    "        fp_acts_pcs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the performance when projecting the actions on any possible subset of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate the model when projecting the actions on a subset of the PCs\n",
    "# best_to_worst removes low variance components first, worst to best remove the \n",
    "# high variance components first\n",
    "n_comp = NUM_MUSCLES\n",
    "num_ep = 10\n",
    "save_results = False\n",
    "target_task_list = [\"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]  # \"early_baoding\",\n",
    "sorting = \"best_to_worst\"  # \"best_to_worst\", \"worst_to_best\"\n",
    "\n",
    "env_config = {\n",
    "        \"env_name\": \"MyoBaodingBallsP1\",\n",
    "        \"weighted_reward_keys\": {\n",
    "            \"pos_dist_1\": 0,\n",
    "            \"pos_dist_2\": 0,\n",
    "            \"act_reg\": 0,\n",
    "            \"solved\": 5,\n",
    "            \"done\": 0,\n",
    "            \"sparse\": 0,\n",
    "        },\n",
    "        # # \"goal_time_period\": [4, 6],  # phase 2: (4, 6)\n",
    "        # # \"goal_xrange\": (0.020, 0.030),  # phase 2: (0.020, 0.030)\n",
    "        # # \"goal_yrange\": (0.022, 0.032),  # phase 2: (0.022, 0.032)\n",
    "        # # # Randomization in physical properties of the baoding balls\n",
    "        \"obj_size_range\": (\n",
    "            0.020,\n",
    "            0.022,\n",
    "        ),  # (0.018, 0.024)   # Object size range. Nominal 0.022\n",
    "        \"obj_mass_range\": (\n",
    "            0.14,\n",
    "            0.16,\n",
    "            # # \"obj_friction_change\": (0.2, 0.001, 0.00002),  # (0.2, 0.001, 0.00002)\n",
    "        ),\n",
    "        \"task_choice\": \"fixed\",\n",
    "        \"seed\": 42\n",
    "    }\n",
    "env_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "model_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "for i in range(2, len(target_task_list) + 1):\n",
    "    target_task_combinations = list(combinations(target_task_list, i))\n",
    "    for target_task_sublist in target_task_combinations:\n",
    "        print(f\"Loading data for tasks {target_task_sublist}...\")\n",
    "        actions_list = []\n",
    "        for target_task in target_task_sublist:\n",
    "            actions = np.vstack(df_dict[target_task].action)\n",
    "            actions_list.append(actions)\n",
    "        actions = np.vstack(actions_list)\n",
    "        \n",
    "        print(f\"Computing pca for tasks {target_task_sublist}...\")\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "        \n",
    "        if save_results:\n",
    "            out_path = os.path.join(ROOT_DIR, \"data\", \"pca\", f\"pca_muscle_act_{'_'.join(target_task_sublist)}.joblib\")\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                joblib.dump(pca, f)\n",
    "\n",
    "        env = EnvironmentFactory.create(**env_config)\n",
    "        vecnormalize = load_vecnormalize(env_path, env)\n",
    "        vecnormalize.training = False\n",
    "        vecnormalize.norm_reward = False\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        print(f\"Running episodes for tasks {target_task_sublist}...\")\n",
    "        performance = []\n",
    "        for k in range(n_comp):\n",
    "            print(\"Environment: \", target_task_sublist, \"component \", k)\n",
    "            if sorting == \"best_to_worst\":\n",
    "                components = pca.components_[:n_comp-k]\n",
    "            elif sorting == \"worst_to_best\":\n",
    "                components = pca.components_[k:]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown sorting: \", sorting)\n",
    "\n",
    "            performance_ep = []\n",
    "            for n in range(num_ep):\n",
    "                acts = []\n",
    "                cum_reward = 0\n",
    "                lstm_states = None\n",
    "                obs = env.reset()\n",
    "                episode_starts = np.ones((1,), dtype=bool)\n",
    "                done = False\n",
    "                timestep = 0\n",
    "                while not done: \n",
    "                    timestep += 1\n",
    "                    action, lstm_states = model.predict(vecnormalize.normalize_obs(obs),\n",
    "                                                            state=lstm_states,\n",
    "                                                            episode_start=episode_starts,\n",
    "                                                            deterministic=True,\n",
    "                                                            )\n",
    "                    \n",
    "                    action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "                    action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "                    obs, rewards, done, info = env.step(action_backproj.reshape(39,))\n",
    "                    episode_starts = done\n",
    "                    cum_reward += rewards\n",
    "                performance_ep.append(cum_reward)\n",
    "                print(f\"Episode {n}, reward: {cum_reward}\")\n",
    "            performance_ep = np.array(performance_ep) / 1000  # Transform the reward into the solved fraction\n",
    "            data_point = {'components': components,'solved_frac_mean': np.mean(performance_ep), 'solved_frac_sem': np.std(performance_ep) / np.sqrt(len(performance_ep))}\n",
    "            performance.append(data_point)\n",
    "        if save_results:\n",
    "            target_task_str = \"_\".join(target_task_sublist)\n",
    "            fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_{target_task_str}_{sorting}.pkl\")\n",
    "            fp_acts_pcs = open(fp, 'wb')\n",
    "            pickle.dump(performance, fp_acts_pcs)\n",
    "            fp_acts_pcs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance curve for the different combinations of tasks\n",
    "def plot_explained_variance_ratio(exp_var, label, color, ax=None, fig=None):\n",
    "    if ax is None or fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.step(range(1, len(exp_var) + 1), exp_var, where='mid', linewidth=3, color=color, label=label)\n",
    "    ax.set_ylabel('Cum. explained variance',fontsize=16)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    return fig, ax\n",
    "\n",
    "def ev(X, X_approx, model_mean):\n",
    "    return 1 - np.sum((X - X_approx)**2) / np.sum((X - model_mean)**2)\n",
    "\n",
    "save_results = False\n",
    "n_comp = NUM_MUSCLES\n",
    "sorting = \"best_to_worst\"\n",
    "target_task_list = [\"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "task_to_print_dict = {\n",
    "    \"baoding\": \"Baoding\",\n",
    "    \"early_baoding\": \"Baoding step 12\",\n",
    "    \"hand_pose\": \"Hand Pose\",\n",
    "    \"hand_reach\": \"Hand Reach\",\n",
    "    \"pen\": \"Pen\",\n",
    "    \"reorient\": \"Reorient\"\n",
    "}\n",
    "muscle_act = np.vstack(df_dict[\"baoding\"].muscle_act)\n",
    "for i in range(2, len(target_task_list) + 1):\n",
    "    target_task_combinations = list(combinations(target_task_list, i))\n",
    "    for target_task_sublist in target_task_combinations:\n",
    "        print(f\"Loading data for tasks {target_task_sublist}...\")\n",
    "        actions_list = []\n",
    "        for target_task in target_task_sublist:\n",
    "            actions = np.vstack(df_dict[target_task].action)\n",
    "            actions_list.append(actions)\n",
    "        actions = np.vstack(actions_list)\n",
    "        \n",
    "        print(f\"Computing pca for tasks {target_task_sublist}...\")\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "        \n",
    "        print(f\"Loading precomputed task performance decay for tasks {target_task_sublist}...\")\n",
    "        target_task_str = \"_\".join(target_task_sublist)\n",
    "        task_perf = pickle.load(open(os.path.join(ROOT_DIR, \"data\", \"performance_decay\", f\"performance_{target_task_str}_{sorting}.pkl\"), \"rb\"))\n",
    "        perfs_mean = np.array([d['solved_frac_mean'] for d in task_perf[::-1]])\n",
    "        perfs_sem = np.array([d['solved_frac_sem'] for d in task_perf[::-1]])\n",
    "        \n",
    "        \n",
    "        muscle_act_projected = pca.transform(muscle_act)\n",
    "        muscle_act_approx = pca.inverse_transform(muscle_act_projected)\n",
    "        exp_var = ev(muscle_act, muscle_act_approx, pca.mean_)\n",
    "        exp_var_ratio_list = [exp_var]\n",
    "        for i in range(1, n_comp):\n",
    "            muscle_act_projected[:, -i:] = 0\n",
    "            muscle_act_approx = pca.inverse_transform(muscle_act_projected)\n",
    "            exp_var = ev(muscle_act, muscle_act_approx, pca.mean_)\n",
    "            exp_var_ratio_list.append(exp_var)\n",
    "        exp_var_ratio_list.reverse()\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax2 = ax1.twinx()\n",
    "        plot_explained_variance_ratio(exp_var_ratio_list, \"Explained variance\", 'black', ax=ax1, fig=fig)\n",
    "        ax2.errorbar(np.arange(1, n_comp + 1), perfs_mean, yerr=perfs_sem, fmt='-', color=\"dodgerblue\")\n",
    "        ax1.set_xlabel('Number of PCs', fontsize=16, labelpad=5)\n",
    "        ax2.set_ylabel('Solved fraction', fontsize=16, labelpad=5, color=\"dodgerblue\")\n",
    "        ax2.tick_params(axis='y', labelsize=14, colors=\"dodgerblue\")\n",
    "        ax1.set_title(\", \".join([task_to_print_dict[task] for task in target_task_sublist]), fontsize=16)\n",
    "        if save_results:\n",
    "            plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"perf_vs_comp_{target_task_str}.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyoChallenge2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
