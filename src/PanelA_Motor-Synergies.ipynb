{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of motor synergies, Todorov's paper (panel A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from definitions import ROOT_DIR\n",
    "import os\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from envs.environment_factory import EnvironmentFactory\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from functions_notebook import PCvsVar,plot_cumvar,make_parallel_envs\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.signal as signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final version - use the newly collected rollouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(ROOT_DIR, \"data\", \"rollouts\")\n",
    "control_data_path = os.path.join(data_dir, \"control\", \"hand_pose_1000_episodes.h5\")\n",
    "control_df = pd.read_hdf(control_data_path)\n",
    "baoding_data_path = os.path.join(data_dir, \"final_model_500_episodes_activations_info_small_variations_ccw\", \"data.hdf\")\n",
    "baoding_df = pd.read_hdf(baoding_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_vel(episode_pos):\n",
    "    episode_vel = np.zeros_like(episode_pos)\n",
    "    episode_vel[1:, :] = (episode_pos[1:] - episode_pos[:-1])  # 40 sim steps per second\n",
    "    # episode_vel = signal.savgol_filter(episode_pos, window_length=3, polyorder=1, deriv=1, axis=0)\n",
    "    return episode_vel\n",
    "\n",
    "def get_pos_vel_act(df):\n",
    "    if \"task\" in df.keys():\n",
    "        pos_list = df.groupby([\"episode\", \"task\"])[\"observation\"].agg(lambda x: np.vstack(x)[:, :23]).tolist()\n",
    "    else:\n",
    "        pos_list = df.groupby([\"episode\"])[\"observation\"].agg(lambda x: np.vstack(x)[:, :23]).tolist()\n",
    "    vel_list = [get_episode_vel(episode_pos) for episode_pos in pos_list]\n",
    "    pos = np.vstack(pos_list)\n",
    "    vel = np.vstack(vel_list)\n",
    "    muscle_act = np.vstack(df.muscle_act)\n",
    "    return pos, vel, muscle_act\n",
    "\n",
    "pos_control, vel_control, muscle_act_control = get_pos_vel_act(control_df)\n",
    "pos_baoding, vel_baoding, muscle_act_baoding = get_pos_vel_act(baoding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of the hand poses for the control (hand pose) and for the task (baoding)\n",
    "num_joints = 23 \n",
    "num_muscles = 39 \n",
    "\n",
    "exp_var_pos_control = PCvsVar(df=pos_control, n_comp=num_joints)\n",
    "exp_var_pos_baoding = PCvsVar(df=pos_baoding, n_comp=num_joints)\n",
    "\n",
    "exp_var_vel_control = PCvsVar(df=vel_control, n_comp=num_joints)\n",
    "exp_var_vel_baoding = PCvsVar(df=vel_baoding, n_comp=num_joints)\n",
    "\n",
    "exp_var_muscle_control = PCvsVar(df=muscle_act_control, n_comp=num_muscles)\n",
    "exp_var_muscle_baoding = PCvsVar(df=muscle_act_baoding, n_comp=num_muscles)\n",
    "\n",
    "exp_var_dict = {\n",
    "    \"pos\": {\n",
    "        \"control\": exp_var_pos_control,\n",
    "        \"baoding\": exp_var_pos_baoding\n",
    "    },\n",
    "    \"vel\": {\n",
    "        \"control\": exp_var_vel_control,\n",
    "        \"baoding\": exp_var_vel_baoding\n",
    "    },\n",
    "    \"muscle_act\": {\n",
    "        \"control\": exp_var_muscle_control,\n",
    "        \"baoding\": exp_var_muscle_baoding\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dof_count(exp_var, threshold=0.85):\n",
    "    cum_exp_var = np.cumsum(exp_var)\n",
    "    for idx, val in enumerate(cum_exp_var):\n",
    "        if val > threshold:\n",
    "            return idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.85, 0.95]\n",
    "dof_count_dict = {}\n",
    "for data_type, task_var_dict in exp_var_dict.items():\n",
    "    dof_per_task_dict = {}\n",
    "    for task, exp_var in task_var_dict.items():\n",
    "        dof_per_level_dict = {}\n",
    "        for l in levels:\n",
    "            dof_count = get_dof_count(exp_var, l)\n",
    "            dof_per_level_dict[l] = dof_count\n",
    "        dof_per_level_dict[\"avg\"] = np.mean(list(dof_per_level_dict.values()))\n",
    "        dof_per_task_dict[task] = dof_per_level_dict\n",
    "    dof_count_dict[data_type] = dof_per_task_dict\n",
    "\n",
    "import json\n",
    "print(json.dumps(dof_count_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from Todorov's paper \n",
    "experimental_dof = {\n",
    "    \"pos\": {\n",
    "        \"control\": {\n",
    "            \"0.85\": 7,\n",
    "            \"0.95\": 10,\n",
    "            \"avg\": 8.5\n",
    "        },\n",
    "        \"baoding\": {\n",
    "            \"0.85\": 3,\n",
    "            \"0.95\": 7,\n",
    "            \"avg\": 5\n",
    "        }\n",
    "    },\n",
    "    \"vel\": {\n",
    "        \"control\": {\n",
    "            \"0.85\": 8,\n",
    "            \"0.95\": 12,\n",
    "            \"avg\": 10\n",
    "        },\n",
    "        \"baoding\": {\n",
    "            \"0.85\": 4,\n",
    "            \"0.95\": 8,\n",
    "            \"avg\": 6\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_compare_explained_variance(exp_var_1, exp_var_2, exp_var_1_label=None, exp_var_2_label=None):\n",
    "    assert len(exp_var_1) == len(exp_var_2)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.step(range(1, len(exp_var_1) + 1), np.cumsum(exp_var_1), where='mid',label=exp_var_1_label, linewidth=3, color=\"dodgerblue\")\n",
    "    ax.step(range(1, len(exp_var_1) + 1), np.cumsum(exp_var_2), where='mid',label=exp_var_2_label, linewidth=3, color=\"orangered\")\n",
    "    ax.set_xlabel('Number of PCs',fontsize=21)\n",
    "    ax.set_ylabel('Cum. explained variance',fontsize=21)\n",
    "    plt.legend(fontsize=21,loc='best')\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.axhline(y=0.95, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=0.85, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.text(18, 0.9, '95%', color = 'black', fontsize=18)\n",
    "    ax.text(18, 0.8, '85%', color = 'black', fontsize=18)\n",
    "    return fig, ax\n",
    "    \n",
    "fig, ax = plot_compare_explained_variance(exp_var_pos_baoding, exp_var_pos_control, \"Baoding\", \"Control (hand pose)\")\n",
    "fig.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_1\", \"pca_pos.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "fig.show()\n",
    "\n",
    "fig, ax = plot_compare_explained_variance(exp_var_vel_baoding, exp_var_vel_control, \"Baoding\", \"Control (hand pose)\")\n",
    "fig.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_1\", \"pca_vel.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "fig, ax = plot_compare_explained_variance(exp_var_muscle_baoding, exp_var_muscle_control, \"Baoding\", \"Control (hand pose)\")\n",
    "fig.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_1\", \"pca_muscle_act.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, while the poses are embedded in a lower dimensional space for the baoding balls task, this is not the case for the muscle activations. In fact, we can hypotesize that the presence of objects and variable environment conditions forces the policy to be more robust, thus preventing the emergence of too stereotypical muscle activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumvar(n_comp=num_joints,exp_var_ratio=exp_var_pos_baoding,title='Joint angular positions')\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_1\", \"pca_pos_baoding_individual_variance.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp=15\n",
    "exp_var_ratio=exp_var_pos_baoding[:15]\n",
    "title='Joint angles'\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.bar(range(1,n_comp+1), exp_var_ratio, alpha=0.5, align='center',label='Individual variance', color=\"dodgerblue\")\n",
    "# plt.xlabel('Number of PCs',fontsize=21)\n",
    "# plt.ylabel('Explained\\nvariance',fontsize=21)\n",
    "# plt.legend(fontsize=21,loc='best')\n",
    "plt.title(title,fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "# plt.xticks([0, 5, 10, 15], fontsize=21)\n",
    "plt.xticks([])\n",
    "plt.subplots_adjust(left=0.15,bottom=0.15)\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_1\", \"cum_var_barplot_sds.png\"), format=\"png\", dpi=800, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_var_ratio_todorov = np.array([35.88, 21.80, 11.31, 7.60, 5.24, 4.21, 3.29, 2.47, 2.06, 1.34, 0.92, 0.72, 0.51, 0.41, 0.30]) * 1e-2\n",
    "n_comp=15\n",
    "exp_var_ratio=exp_var_ratio_todorov\n",
    "# title='Joint angles'\n",
    "plt.figure(figsize=(4, 2))\n",
    "plt.bar(range(1,n_comp+1), exp_var_ratio, alpha=0.5, align='center',label='Individual variance', color=\"dodgerblue\")\n",
    "plt.xlabel('Number of PCs',fontsize=21)\n",
    "# plt.ylabel('Explained\\nvariance',fontsize=21)\n",
    "# plt.legend(fontsize=21,loc='best')\n",
    "# plt.title(title,fontsize=21)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.xticks([0, 5, 10, 15], fontsize=21)\n",
    "plt.subplots_adjust(left=0.15,bottom=0.15)\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_1\", \"cum_var_barplot_human.png\"), format=\"png\", dpi=800, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the DOFs for all datasets\n",
    "baoding_ccw_path = os.path.join(data_dir, \"final_model_500_episodes_activations_info_small_variations_ccw\", \"data.hdf\")\n",
    "ccw_df = pd.read_hdf(baoding_ccw_path)\n",
    "baoding_cw_path = os.path.join(data_dir, \"final_model_500_episodes_activations_info_small_variations_cw\", \"data.hdf\")\n",
    "cw_df = pd.read_hdf(baoding_cw_path)\n",
    "\n",
    "both_df = pd.concat((cw_df, ccw_df), axis=0)\n",
    "\n",
    "pos_cw, vel_cw, muscle_act_cw = get_pos_vel_act(cw_df)\n",
    "pos_ccw, vel_ccw, muscle_act_ccw = get_pos_vel_act(ccw_df)\n",
    "pos_both, vel_both, muscle_act_both = get_pos_vel_act(both_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"small\": {\n",
    "        \"pos\": {\n",
    "            \"cw\": pos_cw,\n",
    "            \"ccw\": pos_ccw,\n",
    "            \"both\": pos_both\n",
    "        },\n",
    "        \"vel\": {\n",
    "            \"cw\": vel_cw,\n",
    "            \"ccw\": vel_ccw,\n",
    "            \"both\": vel_both\n",
    "        },\n",
    "        \"muscle_act\": {\n",
    "            \"cw\": muscle_act_cw,\n",
    "            \"ccw\": muscle_act_ccw,\n",
    "            \"both\": muscle_act_both\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "variation_feature_pca_dict = {}\n",
    "for variation, feature_dict in data_dict.items():\n",
    "    feature_pca_dict = {}\n",
    "    for feature, direction_dict in feature_dict.items():\n",
    "        direction_var_ratio_dict = {}\n",
    "        for direction, dataset in direction_dict.items():\n",
    "            if feature in [\"pos\", \"vel\"]:\n",
    "                exp_var_ratio = PCA(n_components=num_joints).fit(dataset).explained_variance_ratio_\n",
    "            elif feature == \"muscle_act\":\n",
    "                exp_var_ratio = PCA(n_components=num_muscles).fit(dataset).explained_variance_ratio_\n",
    "            direction_var_ratio_dict[direction] = exp_var_ratio\n",
    "        feature_pca_dict[feature] = direction_var_ratio_dict\n",
    "    variation_feature_pca_dict[variation] = feature_pca_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.85, 0.95]\n",
    "for variation, feature_pca_dict in variation_feature_pca_dict.items():\n",
    "    dof_count_dict = {}\n",
    "    for feature, direction_dict in feature_pca_dict.items():\n",
    "        direction_dof_dict = {}\n",
    "        for direction, exp_var in direction_dict.items():\n",
    "            dof_per_level_dict = {}\n",
    "            for l in levels:\n",
    "                dof_count = get_dof_count(exp_var, l)\n",
    "                dof_per_level_dict[l] = dof_count\n",
    "            dof_per_level_dict[\"avg\"] = np.mean(list(dof_per_level_dict.values()))\n",
    "            direction_dof_dict[direction] = dof_per_level_dict\n",
    "        dof_count_dict[feature] = direction_dof_dict\n",
    "\n",
    "    import json\n",
    "    print(json.dumps(dof_count_dict, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the nested dictionary and append DataFrames to the list\n",
    "for feature, directions in dof_count_dict.items():\n",
    "    for direction, thresholds in directions.items():\n",
    "        df = pd.DataFrame([(feature, direction, threshold, value) for threshold, value in thresholds.items()],\n",
    "                          columns=['features', 'direction', 'threshold', 'value'])\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate the list of DataFrames\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Pivot the DataFrame to reshape it\n",
    "df_pivot = df.pivot_table(index='threshold', columns=['features', 'direction'], values='value')\n",
    "\n",
    "# Display the result\n",
    "pretty_print_dict = {\n",
    "    \"cw\": \"CW\",\n",
    "    \"ccw\": \"CCW\",\n",
    "    \"both\": \"Both\",\n",
    "    \"pos\": \"Position\",\n",
    "    \"vel\": \"Velocity\",\n",
    "    \"muscle_act\": \"Muscle activation\"\n",
    "}\n",
    "\n",
    "indices = [(pretty_print_dict[feature], pretty_print_dict[direction]) for feature in [\"pos\", \"vel\", \"muscle_act\"] for direction in [\"cw\", \"ccw\", \"both\"]]\n",
    "\n",
    "df_pivot = df_pivot.rename(columns=pretty_print_dict)\n",
    "print(df_pivot[indices].to_latex(float_format=\"%.1f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the size of the dataset\n",
    "num_joints = 23 \n",
    "num_muscles = 39\n",
    "levels = [0.85, 0.95]\n",
    "\n",
    "percentages = (np.arange(10) + 1) ** 3 / 10\n",
    "\n",
    "data_dict = {\n",
    "    \"pos\": {\n",
    "        \"cw\": pos_cw,\n",
    "        \"ccw\": pos_ccw,\n",
    "        \"both\": pos_both,\n",
    "        \"control\": pos_control\n",
    "    },\n",
    "    \"vel\": {\n",
    "        \"cw\": vel_cw,\n",
    "        \"ccw\": vel_ccw,\n",
    "        \"both\": vel_both,\n",
    "        \"control\": vel_control\n",
    "    },\n",
    "    \"muscle_act\": {\n",
    "        \"cw\": muscle_act_cw,\n",
    "        \"ccw\": muscle_act_ccw,\n",
    "        \"both\": muscle_act_both,\n",
    "        \"control\": muscle_act_control\n",
    "    }\n",
    "}\n",
    "\n",
    "feature_dir_dof_count_dict = {}\n",
    "for feature, dir_dict in data_dict.items():\n",
    "    dir_dof_count_dict = {}\n",
    "    for dir, data in dir_dict.items():\n",
    "        dof_count_list = []\n",
    "        for pct in percentages:\n",
    "            data_pct = data[:int(len(data) * pct // 100)]\n",
    "            if feature in [\"pos\", \"vel\"]:\n",
    "                exp_var = PCvsVar(df=data_pct, n_comp=num_joints)\n",
    "            elif feature == \"muscle_act\":\n",
    "                exp_var = PCvsVar(df=data_pct, n_comp=num_muscles)\n",
    "            else:\n",
    "                raise NotImplementedError(feature)\n",
    "            dof_count = np.mean([get_dof_count(exp_var, l) for l in levels])\n",
    "            dof_count_list.append(dof_count)\n",
    "        dir_dof_count_dict[dir] = dof_count_list\n",
    "    feature_dir_dof_count_dict[feature] = dir_dof_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_dict = {\n",
    "    \"pos\": \"Joint angles\",\n",
    "    \"vel\": \"Joint angular velocities\",\n",
    "    \"muscle_act\": \"Muscle activations\",\n",
    "    \"ccw\": \"Baoding CCW\",\n",
    "    \"cw\": \"Baoding CW\",\n",
    "    \"both\": \"Baoding CW and CCW\",\n",
    "    \"control\": \"Control\"\n",
    "}\n",
    "max_episodes_dict = {\n",
    "    \"ccw\": 500,\n",
    "    \"cw\": 500,\n",
    "    \"both\": 1000,\n",
    "    \"control\": 1000\n",
    "}\n",
    "\n",
    "for dataset in [\"cw\", \"ccw\", \"both\", \"control\"]:\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "    for feature in [\"pos\", \"vel\", \"muscle_act\"]:\n",
    "        x = percentages * max_episodes_dict[dataset] / 100\n",
    "        y = feature_dir_dof_count_dict[feature][dataset]\n",
    "        ax.plot(x, y, \".-\", label=legend_dict[feature])\n",
    "    ax.set_title(legend_dict[dataset])\n",
    "    ax.set_xlabel(\"Number of episodes\", fontsize=12)\n",
    "    ax.set_ylabel(\"Estimated dimensionality\", fontsize=12)\n",
    "    # ax.legend(bbox_to_anchor=(1, 1))\n",
    "    fig.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_1\", f\"dataset_size_validation_{dataset}.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. baoding task : hand pose lattice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the hand's positions and velocities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the explained variance vs. number of PCs for each 3 pre-processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file from Basecamp : 'rollouts_hand_pose_lattice.h5'\n",
    "fp = os.path.join(ROOT_DIR,'rollouts_hand_pose_lattice.h5')\n",
    "data_dict = pd.read_hdf(fp).to_dict(orient='dict')\n",
    "observations = data_dict['observation']\n",
    "hand_positions = np.array([observations[ep][0:23] for ep in observations.keys()])\n",
    "hand_velocities = np.array([observations[ep][23:46] for ep in observations.keys()])\n",
    "muscle_act = data_dict[\"muscle_act\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compute the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 23\n",
    "\n",
    "# Absolute joint angles\n",
    "abs_pos_var = PCvsVar(df=hand_positions, n_comp=n)\n",
    "print('Abs, position\\n',np.cumsum(abs_pos_var))\n",
    "\n",
    "# Normalized to range (0,1)\n",
    "norm01_hand_positions = np.array([(hand_pos-np.min(hand_pos))/(np.max(hand_pos)-np.min(hand_pos)) for hand_pos in hand_positions.T]).T\n",
    "norm01_pos_var = PCvsVar(df=norm01_hand_positions, n_comp=n)\n",
    "print('0-1 range, position \\n',np.cumsum(norm01_pos_var))\n",
    "\n",
    "# Normalized to unit variance\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "norm_hand_positions = np.array([np.squeeze(scaler.fit_transform(np.reshape(hand_pos,(hand_positions.shape[0],1)))) for hand_pos in hand_positions.T]).T\n",
    "norm_pos_var = PCvsVar(df=norm_hand_positions, n_comp=n)\n",
    "print('Unit variance, position \\n',np.cumsum(norm_pos_var))\n",
    "\n",
    "# Absolute joint velocities\n",
    "abs_vel_var = PCvsVar(df=hand_velocities, n_comp=n)\n",
    "print('Abs, velocity\\n',np.cumsum(abs_vel_var))\n",
    "\n",
    "# Normalized to range (0,1)\n",
    "norm01_hand_velocities = np.array([(hand_vel-np.min(hand_vel))/(np.max(hand_vel)-np.min(hand_vel)) for hand_vel in hand_velocities.T]).T\n",
    "norm01_vel_var = PCvsVar(df=norm01_hand_velocities, n_comp=n)\n",
    "print('0-1 range, velocity\\n',np.cumsum(norm01_vel_var))\n",
    "\n",
    "# Normalized to unit variance\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "norm_hand_velocities = np.array([np.squeeze(scaler.fit_transform(np.reshape(hand_vel,(hand_velocities.shape[0],1)))) for hand_vel in hand_velocities.T]).T\n",
    "norm_vel_var = PCvsVar(df=norm_hand_velocities, n_comp=n)\n",
    "print('Unit variance, velocity\\n',np.cumsum(norm_vel_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the explained variance vs. number of PCs for each 3 pre-processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position space\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=abs_pos_var,title='Absolute joint angles, baoding')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm01_pos_var,title='[0,1] normalized joint angles, baoding')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm_pos_var,title='Unit variance normalized joint angles, baoding')\n",
    "\n",
    "# Velocity space\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=abs_vel_var,title='Absolute joint velocities, baoding')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm01_vel_var,title='[0,1] normalized joint velocities, baoding')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm_vel_var,title='Unit variance normalized joint velocities, baoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Manipulation task : baoding balls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate and save the hand's positions and velocities. __Go to 2. to directly load the previously-obtained data.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CustomMyoBaodingBallsP2' \n",
    "render = False\n",
    "\n",
    "PATH_TO_NORMALIZED_ENV = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "PATH_TO_PRETRAINED_NET = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"weighted_reward_keys\": {\n",
    "        \"pos_dist_1\": 0,\n",
    "        \"pos_dist_2\": 0,\n",
    "        \"act_reg\": 0,\n",
    "        \"alive\": 0,\n",
    "        \"solved\": 5,\n",
    "        \"done\": 0,\n",
    "        \"sparse\": 0\n",
    "    },\n",
    "    \"enable_rsi\": False,\n",
    "    \"rsi_probability\": 0,\n",
    "    \"balls_overlap\": False,\n",
    "    \"overlap_probability\": 0,\n",
    "    \"noise_fingers\": 0,\n",
    "    \"limit_init_angle\": 3.141592653589793,\n",
    "    \"goal_time_period\": [\n",
    "        5,\n",
    "        5\n",
    "    ],\n",
    "    \"goal_xrange\": [\n",
    "        0.02,\n",
    "        0.03\n",
    "    ],\n",
    "    \"goal_yrange\": [\n",
    "        0.022,\n",
    "        0.032\n",
    "    ],\n",
    "    \"obj_size_range\": [\n",
    "        0.018,\n",
    "        0.024\n",
    "    ],\n",
    "    \"obj_mass_range\": [\n",
    "        0.03,\n",
    "        0.3\n",
    "    ],\n",
    "    \"obj_friction_change\": [\n",
    "        0.2,\n",
    "        0.001,\n",
    "        2e-05\n",
    "    ],\n",
    "    \"task_choice\": \"random\",\n",
    "    # \"rotation_direction\" : \"cw\"\n",
    "}\n",
    "\n",
    "envs = make_parallel_envs(env_name, config, num_env=1)\n",
    "envs = VecNormalize.load(PATH_TO_NORMALIZED_ENV, envs)\n",
    "envs.training = False\n",
    "envs.norm_reward = False\n",
    "custom_objects = {\n",
    "    \"learning_rate\": lambda _: 0,\n",
    "    \"lr_schedule\": lambda _: 0,\n",
    "    \"clip_range\": lambda _: 0,\n",
    "}\n",
    "model = RecurrentPPO.load(\n",
    "        PATH_TO_PRETRAINED_NET, env=envs, device=\"cpu\", custom_objects=custom_objects\n",
    "    )\n",
    "\n",
    "eval_model = model\n",
    "eval_env = EnvironmentFactory.create(env_name,**config)\n",
    "\n",
    "num_ep = 50\n",
    "tot_pos = []\n",
    "tot_vel = []\n",
    "tot_act = []\n",
    "\n",
    "for n in range(num_ep):\n",
    "    obs_tot = []\n",
    "    act_tot = []\n",
    "    cum_reward = 0\n",
    "    lstm_states = None\n",
    "    obs = eval_env.reset()\n",
    "    episode_starts = np.ones((1,), dtype=bool)\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    # while not done: \n",
    "    for i in range(200):  # DO NOT STOP WHEN THE BALLS FALL\n",
    "        if render :\n",
    "            eval_env.sim.render(mode=\"window\")\n",
    "            \n",
    "        timestep += 1\n",
    "        action, lstm_states = eval_model.predict(envs.normalize_obs(obs),\n",
    "                                                state=lstm_states,\n",
    "                                                episode_start=episode_starts,\n",
    "                                                deterministic=True,\n",
    "                                                )\n",
    "                                                    \n",
    "        obs, rewards, done, info = eval_env.step(action)\n",
    "        episode_starts = done\n",
    "        cum_reward += rewards\n",
    "        obs_tot.append(obs)\n",
    "        act_tot.append(eval_env.last_ctrl)\n",
    "    print(cum_reward)\n",
    "\n",
    "    # MEASURE JOINT POSITION AND VELOCITY\n",
    "    hand_positions = np.array(obs_tot)[:,0:23]    \n",
    "    hand_velocities = np.array([np.diff(pos)/0.0025 for pos in hand_positions.T]).T\n",
    "    hand_velocities = np.vstack((np.zeros((1,23)),hand_velocities))\n",
    "    muscle_act = np.array(act_tot)\n",
    "    tot_pos.append(hand_positions)\n",
    "    tot_vel.append(hand_velocities)\n",
    "    tot_act.append(muscle_act)\n",
    "    \n",
    "tot_pos = np.concatenate(tot_pos, axis=0)\n",
    "tot_vel = np.concatenate(tot_vel, axis=0)\n",
    "tot_act = np.concatenate(tot_act, axis=0)\n",
    "\n",
    "# out_path = os.path.join(ROOT_DIR, \"data\", \"basecamp\", \"posvel_synergies_10k.pkl\")\n",
    "# with open(out_path, 'wb') as file:\n",
    "#     pickle.dump([tot_pos, tot_vel, tot_act], file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the hand's positions and velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the file from basecamp : 'posvel_synergies'\n",
    "l = pickle.load(open('/home/ingster/Bureau/SIL-BigResults/posvel_synergies','rb'))\n",
    "hand_positions = l[0]\n",
    "hand_velocities = l[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compute the PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 23\n",
    "\n",
    "# Absolute joint angles\n",
    "abs_pos_var = PCvsVar(df=hand_positions, n_comp=n)\n",
    "print('Abs, position\\n',np.cumsum(abs_pos_var))\n",
    "\n",
    "# Normalized to range (0,1)\n",
    "norm01_hand_positions = np.array([(hand_pos-np.min(hand_pos))/(np.max(hand_pos)-np.min(hand_pos)) for hand_pos in hand_positions.T]).T\n",
    "norm01_pos_var = PCvsVar(df=norm01_hand_positions, n_comp=n)\n",
    "print('0-1 range, position \\n',np.cumsum(norm01_pos_var))\n",
    "\n",
    "# Normalized to unit variance\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "norm_hand_positions = np.array([np.squeeze(scaler.fit_transform(np.reshape(hand_pos,(hand_positions.shape[0],1)))) for hand_pos in hand_positions.T]).T\n",
    "norm_pos_var = PCvsVar(df=norm_hand_positions, n_comp=n)\n",
    "print('Unit variance, position \\n',np.cumsum(norm_pos_var))\n",
    "\n",
    "# Absolute joint velocities\n",
    "abs_vel_var = PCvsVar(df=hand_velocities, n_comp=n)\n",
    "print('Abs, velocity\\n',np.cumsum(abs_vel_var))\n",
    "\n",
    "# Normalized to range (0,1)\n",
    "norm01_hand_velocities = np.array([(hand_vel-np.min(hand_vel))/(np.max(hand_vel)-np.min(hand_vel)) for hand_vel in hand_velocities.T]).T\n",
    "norm01_vel_var = PCvsVar(df=norm01_hand_velocities, n_comp=n)\n",
    "print('0-1 range, velocity\\n',np.cumsum(norm01_vel_var))\n",
    "\n",
    "# Normalized to unit variance\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "norm_hand_velocities = np.array([np.squeeze(scaler.fit_transform(np.reshape(hand_vel,(hand_velocities.shape[0],1)))) for hand_vel in hand_velocities.T]).T\n",
    "norm_vel_var = PCvsVar(df=norm_hand_velocities, n_comp=n)\n",
    "print('Unit variance, velocity\\n',np.cumsum(norm_vel_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the explained variance vs. number of PCs for each 3 pre-processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position space\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=abs_pos_var,title='Absolute joint angles')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm01_pos_var,title='[0,1] normalized joint angles')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm_pos_var,title='Unit variance normalized joint angles')\n",
    "\n",
    "# Velocity space\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=abs_vel_var,title='Absolute joint velocities')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm01_vel_var,title='[0,1] normalized joint velocities')\n",
    "plot_cumvar(n_comp=n,exp_var_ratio=norm_vel_var,title='Unit variance normalized joint velocities')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyoChallenge2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
