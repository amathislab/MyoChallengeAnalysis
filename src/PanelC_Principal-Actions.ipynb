{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of principal actions (panel C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import pickle\n",
    "from definitions import ROOT_DIR\n",
    "from sklearn.decomposition import PCA\n",
    "from helpers import make_parallel_envs, set_config\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from envs.environment_factory import EnvironmentFactory\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from main_eval import load_vecnormalize, load_model\n",
    "from matplotlib.cm import get_cmap\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small variation datasets\n",
    "df_name_dict = {\n",
    "    \"baoding\": \"baoding_sv_no_activity.h5\",\n",
    "    \"early_baoding\": \"baoding_step_12_no_activity.h5\",\n",
    "    \"hand_pose\": \"hand_pose.h5\",\n",
    "    \"hand_reach\": \"hand_reach.h5\",\n",
    "    \"pen\": \"pen.h5\",\n",
    "    \"reorient\": \"reorient.h5\",\n",
    "\n",
    "}\n",
    "df_dict = {\n",
    "    key: pd.read_hdf(os.path.join(ROOT_DIR, \"data\", \"datasets\", value))\n",
    "    for key, value in df_name_dict.items()\n",
    "}\n",
    "# baoding_df = pd.read_hdf(os.path.join(ROOT_DIR, \"data\", \"datasets\", \"baoding_sv_no_activity.h5\"))\n",
    "# control_df = pd.read_hdf(os.path.join(ROOT_DIR, \"data\", \"datasets\", \"hand_pose.h5\"))\n",
    "\n",
    "\n",
    "# cw_path = os.path.join(ROOT_DIR, \"data\", \"rollouts\", \"final_model_500_episodes_activations_info_small_variations_cw\", \"data.hdf\")\n",
    "# rollouts_cw = pd.read_hdf(cw_path)\n",
    "# ccw_path = os.path.join(ROOT_DIR, \"data\", \"rollouts\", \"final_model_500_episodes_activations_info_small_variations_ccw\", \"data.hdf\")\n",
    "# rollouts_ccw = pd.read_hdf(ccw_path)\n",
    "# rollouts_df = pd.concat((rollouts_cw, rollouts_ccw)).reset_index()\n",
    "# # rollouts_df = rollouts_ccw\n",
    "\n",
    "# cw_path = os.path.join(ROOT_DIR, \"data\", \"rollouts\", \"step_12_500_episodes_activations_info_cw\", \"data.hdf\")\n",
    "# early_baoding_rollouts_cw = pd.read_hdf(cw_path)\n",
    "# ccw_path = os.path.join(ROOT_DIR, \"data\", \"rollouts\", \"step_12_500_episodes_activations_info_ccw\", \"data.hdf\")\n",
    "# early_baoding_rollouts_ccw = pd.read_hdf(ccw_path)\n",
    "# early_baoding_rollouts_df = pd.concat((early_baoding_rollouts_cw, early_baoding_rollouts_ccw)).reset_index()\n",
    "# # early_baoding_rollouts_df = early_baoding_rollouts_ccw\n",
    "\n",
    "# data_dir = os.path.join(ROOT_DIR, \"data\", \"rollouts\")\n",
    "# control_tasks_dict = {\n",
    "#     \"hand_pose\": \"hand_pose_1000_episodes_lattice.h5\",\n",
    "#     \"hand_reach\": \"hand_reach_1000_episodes_lattice.h5\",\n",
    "#     \"reorient\": \"reorient_1000_episodes_lattice.h5\",\n",
    "#     \"pen\": \"pen_1000_episodes_lattice.h5\",\n",
    "# }\n",
    "# df_dict = {\n",
    "#     key: pd.read_hdf(os.path.join(data_dir, \"control\", value))\n",
    "#     for key, value in control_tasks_dict.items()\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate the model when projecting the actions on a subset of the PCs\n",
    "# best_to_worst removes low variance components first, worst to best remove the \n",
    "# high variance components first\n",
    "n_comp = 39\n",
    "num_ep = 100\n",
    "target_task_list = [\"early_baoding\"]  # , \"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "sorting = \"best_to_worst\"  # \"best_to_worst\", \"worst_to_best\"\n",
    "env_config = {\n",
    "        \"env_name\": \"MyoBaodingBallsP1\",\n",
    "        \"weighted_reward_keys\": {\n",
    "            \"pos_dist_1\": 0,\n",
    "            \"pos_dist_2\": 0,\n",
    "            \"act_reg\": 0,\n",
    "            \"solved\": 5,\n",
    "            \"done\": 0,\n",
    "            \"sparse\": 0,\n",
    "        },\n",
    "        # # \"goal_time_period\": [4, 6],  # phase 2: (4, 6)\n",
    "        # # \"goal_xrange\": (0.020, 0.030),  # phase 2: (0.020, 0.030)\n",
    "        # # \"goal_yrange\": (0.022, 0.032),  # phase 2: (0.022, 0.032)\n",
    "        # # # Randomization in physical properties of the baoding balls\n",
    "        \"obj_size_range\": (\n",
    "            0.020,\n",
    "            0.022,\n",
    "        ),  # (0.018, 0.024)   # Object size range. Nominal 0.022\n",
    "        \"obj_mass_range\": (\n",
    "            0.14,\n",
    "            0.16,\n",
    "            # # \"obj_friction_change\": (0.2, 0.001, 0.00002),  # (0.2, 0.001, 0.00002)\n",
    "        ),\n",
    "        \"task_choice\": \"fixed\",\n",
    "        \"seed\": 42\n",
    "    }\n",
    "env_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "model_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "for target_task in target_task_list:\n",
    "    # First choose the pca to use to project the actions\n",
    "    if target_task == \"baoding\":\n",
    "        actions = np.vstack(rollouts_df.action)\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "    elif target_task == \"early_baoding\":\n",
    "        actions = np.vstack(early_baoding_rollouts_df.action)\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "    else:\n",
    "        pca = joblib.load(os.path.join(ROOT_DIR, \"data\", \"pca\", f\"pca_muscle_act_{target_task}.joblib\"))\n",
    "\n",
    "\n",
    "    env = EnvironmentFactory.create(**env_config)\n",
    "    vecnormalize = load_vecnormalize(env_path, env)\n",
    "    vecnormalize.training = False\n",
    "    vecnormalize.norm_reward = False\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    performance = []\n",
    "    for k in range(n_comp):\n",
    "        print(\"Environment: \", target_task, \"component \", k)\n",
    "        if sorting == \"best_to_worst\":\n",
    "            components = pca.components_[:n_comp-k]\n",
    "        elif sorting == \"worst_to_best\":\n",
    "            components = pca.components_[k:]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sorting: \", sorting)\n",
    "\n",
    "        performance_ep = []\n",
    "        for n in range(num_ep):\n",
    "            acts = []\n",
    "            cum_reward = 0\n",
    "            lstm_states = None\n",
    "            obs = env.reset()\n",
    "            episode_starts = np.ones((1,), dtype=bool)\n",
    "            done = False\n",
    "            timestep = 0\n",
    "            while not done: \n",
    "                timestep += 1\n",
    "                action, lstm_states = model.predict(vecnormalize.normalize_obs(obs),\n",
    "                                                        state=lstm_states,\n",
    "                                                        episode_start=episode_starts,\n",
    "                                                        deterministic=True,\n",
    "                                                        )\n",
    "                \n",
    "                action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "                action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "                obs, rewards, done, info = env.step(action_backproj.reshape(39,))\n",
    "                episode_starts = done\n",
    "                cum_reward += rewards\n",
    "            performance_ep.append(cum_reward)\n",
    "            print(f\"Episode {n}, reward: {cum_reward}\")\n",
    "        performance_ep = np.array(performance_ep) / 1000  # Transform the reward into the solved fraction\n",
    "        data_point = {'components':components,'solved_frac_mean': np.mean(performance_ep), 'solved_frac_sem': np.std(performance_ep) / np.sqrt(len(performance_ep))}\n",
    "        performance.append(data_point)\n",
    "        print(data_point)\n",
    "\n",
    "    fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_action_components_{target_task}_{sorting}.pkl\")\n",
    "    fp_acts_pcs = open(fp, 'wb')\n",
    "    pickle.dump(performance, fp_acts_pcs)\n",
    "    fp_acts_pcs.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the performances vs. number of principal actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_modes = [\"best_to_worst\", \"worst_to_best\"]\n",
    "target_task_list = [\"baoding\", \"early_baoding\", \"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "task_to_print_dict = {\n",
    "    \"baoding\": \"Baoding\",\n",
    "    \"early_baoding\": \"Baoding step 12\",\n",
    "    \"hand_pose\": \"Hand Pose\",\n",
    "    \"hand_reach\": \"Hand Reach\",\n",
    "    \"pen\": \"Pen\",\n",
    "    \"reorient\": \"Reorient\"\n",
    "}\n",
    "task_perf_dict = {}\n",
    "for target_task in target_task_list:\n",
    "    task_dict = {\n",
    "        mode: pickle.load(open(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_action_components_{target_task}_{mode}.pkl\"),'rb')) for mode in sorting_modes\n",
    "    }\n",
    "    task_perf_dict[target_task] = task_dict\n",
    "n_comp = 39"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the performance vs. number of dimensions removed in the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file from Basecamp : 'performance_actions_components_t'\n",
    "sorting_modes = [\"best_to_worst\", \"worst_to_best\"]\n",
    "\n",
    "perf_dict = {\n",
    "        mode: pickle.load(open(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_action_components_baoding_{mode}.pkl\"),'rb')) for mode in sorting_modes\n",
    "    }\n",
    "\n",
    "n_comp = 39\n",
    "\n",
    "label_dict = {\n",
    "    \"best_to_worst\": \"High to low EV\",\n",
    "    \"worst_to_best\": \"Low to high EV\",\n",
    "}\n",
    "color_list = [\"dodgerblue\", \"red\"]\n",
    "plt.figure()\n",
    "for (mode, performance_components), color in zip(perf_dict.items(), color_list):\n",
    "    perfs_mean = np.array([d['solved_frac_mean'] for d in performance_components[::-1]])\n",
    "    perfs_sem = np.array([d['solved_frac_sem'] for d in performance_components[::-1]])\n",
    "\n",
    "    # perfs_max = perfs_mean + perfs_sem\n",
    "    # perfs_min = perfs_mean - perfs_sem\n",
    "    # plt.plot(np.arange(n_comp), perfs_mean, linewidth=1, label=mode)\n",
    "    # plt.fill_between(np.arange(n_comp), perfs_min, perfs_max, color='gray', alpha=0.3)\n",
    "    plt.errorbar(np.arange(n_comp), perfs_mean, yerr=perfs_sem, fmt='-', label=label_dict[mode], color=color)\n",
    "    plt.xlabel('Number of PCs',fontsize=16,labelpad=5)\n",
    "    plt.ylabel('Solved fraction',fontsize=16,labelpad=5)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.subplots_adjust(left=0.2,bottom=0.23)\n",
    "plt.legend(bbox_to_anchor=(1.02, 1.22), fontsize=14, ncol=2)\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"pca_performance_contrib.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should we also add the cumulative explained variance?\n",
    "label_dict = {\n",
    "    \"best_to_worst\": \"High to low EV\",\n",
    "    \"worst_to_best\": \"Low to high EV\",\n",
    "}\n",
    "task_colors = {\n",
    "    \"baoding\": \"dodgerblue\",\n",
    "    \"early_baoding\": \"blue\",\n",
    "    \"pen\": \"blueviolet\",\n",
    "    \"reorient\": \"violet\",\n",
    "    \"hand_reach\": \"orange\",\n",
    "    \"hand_pose\": \"red\",\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "for task, perf_dict in task_perf_dict.items():\n",
    "    mode = \"best_to_worst\"\n",
    "    performance_components = perf_dict[mode]\n",
    "    color = task_colors[task]\n",
    "    perfs_mean = np.array([d['solved_frac_mean'] for d in performance_components[::-1]])\n",
    "    perfs_sem = np.array([d['solved_frac_sem'] for d in performance_components[::-1]])\n",
    "\n",
    "    # perfs_max = perfs_mean + perfs_sem\n",
    "    # perfs_min = perfs_mean - perfs_sem\n",
    "    # plt.plot(np.arange(n_comp), perfs_mean, linewidth=1, label=mode)\n",
    "    # plt.fill_between(np.arange(n_comp), perfs_min, perfs_max, color='gray', alpha=0.3)\n",
    "    plt.errorbar(np.arange(n_comp), perfs_mean, yerr=perfs_sem, fmt='-', label=task_to_print_dict[task], color=color)\n",
    "plt.xlabel('Number of PCs (muscles)',fontsize=19,labelpad=5)\n",
    "plt.ylabel('Solved fraction',fontsize=19,labelpad=5)\n",
    "plt.yticks(fontsize=19)\n",
    "plt.xticks(np.arange(0, 41, 10), fontsize=19)\n",
    "plt.xlim((-1, 41))\n",
    "plt.subplots_adjust(left=0.2,bottom=0.23)\n",
    "# plt.legend(bbox_to_anchor=(1, 0.9), fontsize=14)\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"pca_performance_contrib.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate step 12 on all the other PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate the model when projecting the actions on a subset of the PCs\n",
    "# best_to_worst removes low variance components first, worst to best remove the \n",
    "# high variance components first\n",
    "n_comp = 39\n",
    "num_ep = 10\n",
    "target_task_list = [\"baoding\", \"early_baoding\"]  #, \"early_baoding\", \"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "sorting = \"best_to_worst\"  # \"best_to_worst\", \"worst_to_best\"\n",
    "env_config = {\n",
    "        \"env_name\": \"CustomMyoBaodingBallsP1\",\n",
    "        \"weighted_reward_keys\": {\n",
    "            \"pos_dist_1\": 0,\n",
    "            \"pos_dist_2\": 0,\n",
    "            \"act_reg\": 0,\n",
    "            \"solved\": 5,\n",
    "            \"done\": 0,\n",
    "            \"sparse\": 0,\n",
    "        },\n",
    "        \"task_choice\": \"fixed\",\n",
    "        \"goal_time_period\": (5, 5),\n",
    "        \"seed\": 42\n",
    "    }\n",
    "env_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/12_period_5/env.pkl\",\n",
    ")\n",
    "model_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/12_period_5/model.zip\",\n",
    ")\n",
    "\n",
    "\n",
    "for target_task in target_task_list:\n",
    "    # First choose the pca to use to project the actions\n",
    "    if target_task == \"baoding\":\n",
    "        actions = np.vstack(rollouts_df.action)\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "    elif target_task == \"early_baoding\":\n",
    "        actions = np.vstack(early_baoding_rollouts_df.action)\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "    else:\n",
    "        pca = joblib.load(os.path.join(ROOT_DIR, \"data\", \"pca\", f\"pca_muscle_act_{target_task}.joblib\"))\n",
    "\n",
    "    env = EnvironmentFactory.create(**env_config)\n",
    "    vecnormalize = load_vecnormalize(env_path, env)\n",
    "    vecnormalize.training = False\n",
    "    vecnormalize.norm_reward = False\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    performance = []\n",
    "    for k in range(n_comp):\n",
    "        print(\"Environment: \", target_task, \"component \", k)\n",
    "        if sorting == \"best_to_worst\":\n",
    "            components = pca.components_[:n_comp-k]\n",
    "        elif sorting == \"worst_to_best\":\n",
    "            components = pca.components_[k:]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sorting: \", sorting)\n",
    "\n",
    "        performance_ep = []\n",
    "        for n in range(num_ep):\n",
    "            acts = []\n",
    "            cum_reward = 0\n",
    "            lstm_states = None\n",
    "            obs = env.reset(random_phase=0)\n",
    "            episode_starts = np.ones((1,), dtype=bool)\n",
    "            done = False\n",
    "            timestep = 0\n",
    "            while not done: \n",
    "                timestep += 1\n",
    "                action, lstm_states = model.predict(vecnormalize.normalize_obs(obs),\n",
    "                                                        state=lstm_states,\n",
    "                                                        episode_start=episode_starts,\n",
    "                                                        deterministic=True,\n",
    "                                                        )\n",
    "                \n",
    "                action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "                action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "                obs, rewards, done, info = env.step(action_backproj.reshape(39,))\n",
    "                episode_starts = done\n",
    "                cum_reward += rewards\n",
    "            performance_ep.append(cum_reward)\n",
    "            print(f\"Episode {n}, reward: {cum_reward}\")\n",
    "        performance_ep = np.array(performance_ep) / 1000  # Transform the reward into the solved fraction\n",
    "        data_point = {'components':components,'solved_frac_mean': np.mean(performance_ep), 'solved_frac_sem': np.std(performance_ep) / np.sqrt(len(performance_ep))}\n",
    "        performance.append(data_point)\n",
    "        print(data_point)\n",
    "\n",
    "    fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_step_12_{target_task}_{sorting}.pkl\")\n",
    "    fp_acts_pcs = open(fp, 'wb')\n",
    "    pickle.dump(performance, fp_acts_pcs)\n",
    "    fp_acts_pcs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_modes = [\"best_to_worst\"]\n",
    "target_task_list = [\"baoding\", \"early_baoding\", \"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "task_to_print_dict = {\n",
    "    \"baoding\": \"Baoding\",\n",
    "    \"early_baoding\": \"Baoding step 12\",\n",
    "    \"hand_pose\": \"Hand Pose\",\n",
    "    \"hand_reach\": \"Hand Reach\",\n",
    "    \"pen\": \"Pen\",\n",
    "    \"reorient\": \"Reorient\"\n",
    "}\n",
    "task_perf_dict = {}\n",
    "for target_task in target_task_list:\n",
    "    task_dict = {\n",
    "        sorting: pickle.load(open(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_step_12_{target_task}_{sorting}.pkl\"),'rb')) for sorting in sorting_modes\n",
    "    }\n",
    "    task_perf_dict[target_task] = task_dict\n",
    "n_comp = 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    \"best_to_worst\": \"High to low EV\",\n",
    "    \"worst_to_best\": \"Low to high EV\",\n",
    "}\n",
    "\n",
    "task_colors = {\n",
    "    \"baoding\": \"dodgerblue\",\n",
    "    \"early_baoding\": \"blue\",\n",
    "    \"pen\": \"blueviolet\",\n",
    "    \"reorient\": \"violet\",\n",
    "    \"hand_reach\": \"orange\",\n",
    "    \"hand_pose\": \"red\",\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "for task, perf_dict in task_perf_dict.items():\n",
    "    mode = \"best_to_worst\"\n",
    "    performance_components = perf_dict[mode]\n",
    "    color = task_colors[task]\n",
    "    perfs_mean = np.array([d['solved_frac_mean'] for d in performance_components[::-1]])\n",
    "    perfs_sem = np.array([d['solved_frac_sem'] for d in performance_components[::-1]])\n",
    "\n",
    "    plt.errorbar(np.arange(n_comp), perfs_mean, yerr=perfs_sem, fmt='-', label=task_to_print_dict[task], color=color)\n",
    "plt.xlabel('Number of PCs (muscles)',fontsize=19,labelpad=5)\n",
    "plt.ylabel('Solved fraction',fontsize=19,labelpad=5)\n",
    "plt.yticks(fontsize=19)\n",
    "plt.xticks(np.arange(0, 41, 10), fontsize=19)\n",
    "plt.xlim((-1, 41))\n",
    "plt.subplots_adjust(left=0.2,bottom=0.23)\n",
    "# plt.legend(bbox_to_anchor=(1, 0.9), fontsize=14)\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"pca_performance_contrib_step_12.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance curve for the different combinations of tasks\n",
    "sorting = \"best_to_worst\"\n",
    "task_to_print_dict = {\n",
    "    \"baoding\": \"Baoding\",\n",
    "    \"early_baoding\": \"Baoding step 12\",\n",
    "    \"hand_pose\": \"Hand Pose\",\n",
    "    \"hand_reach\": \"Hand Reach\",\n",
    "    \"pen\": \"Pen\",\n",
    "    \"reorient\": \"Reorient\"\n",
    "}\n",
    "fp = fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_step_12_{target_task}_{sorting}.pkl\")\n",
    "task_perf = pickle.load(open(fp, \"rb\"))\n",
    "perfs_mean = np.array([d['solved_frac_mean'] for d in task_perf[::-1]])\n",
    "perfs_sem = np.array([d['solved_frac_sem'] for d in task_perf[::-1]])\n",
    "\n",
    "plt.errorbar(np.arange(n_comp), perfs_mean, yerr=perfs_sem, fmt='-')\n",
    "plt.xlabel('Number of PCs',fontsize=16,labelpad=5)\n",
    "plt.ylabel('Solved fraction',fontsize=16,labelpad=5)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.subplots_adjust(left=0.2,bottom=0.23)\n",
    "plt.title(\"Baoding step 12 of final Baoding\")\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"perf_vs_comp_baoding_step_12_on_baoding.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: does the space spanned by early_baoding work if there are no perturbations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate the model when projecting the actions on a subset of the PCs\n",
    "# best_to_worst removes low variance components first, worst to best remove the \n",
    "# high variance components first\n",
    "n_comp = 39\n",
    "num_ep = 5\n",
    "target_task_list = [\"early_baoding\"]  # , \"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "sorting = \"worst_to_best\"  # \"best_to_worst\", \"worst_to_best\"\n",
    "env_config = {\n",
    "        \"env_name\": \"MyoBaodingBallsP1\",\n",
    "        \"weighted_reward_keys\": {\n",
    "            \"pos_dist_1\": 0,\n",
    "            \"pos_dist_2\": 0,\n",
    "            \"act_reg\": 0,\n",
    "            \"solved\": 5,\n",
    "            \"done\": 0,\n",
    "            \"sparse\": 0,\n",
    "        },\n",
    "        \"goal_time_period\": [5, 5],  # phase 2: (4, 6)\n",
    "        \"goal_xrange\": (0.025, 0.025),  # phase 2: (0.020, 0.030)\n",
    "        \"goal_yrange\": (0.028, 0.028),  # phase 2: (0.022, 0.032)\n",
    "        # # # Randomization in physical properties of the baoding balls\n",
    "        \"obj_size_range\": (\n",
    "            0.022,\n",
    "            0.022,\n",
    "        ),  # (0.018, 0.024)   # Object size range. Nominal 0.022\n",
    "        \"obj_mass_range\": (\n",
    "            0.043,\n",
    "            0.043,\n",
    "            # # \"obj_friction_change\": (0.2, 0.001, 0.00002),  # (0.2, 0.001, 0.00002)\n",
    "        ),\n",
    "        \"task_choice\": \"fixed\",\n",
    "        \"seed\": 42\n",
    "    }\n",
    "env_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "model_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "for target_task in target_task_list:\n",
    "    # First choose the pca to use to project the actions\n",
    "    if target_task == \"baoding\":\n",
    "        actions = np.vstack(rollouts_df.action)\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "    elif target_task == \"early_baoding\":\n",
    "        actions = np.vstack(early_baoding_rollouts_df.action)\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "    else:\n",
    "        pca = joblib.load(os.path.join(ROOT_DIR, \"data\", \"pca\", f\"pca_muscle_act_{target_task}.joblib\"))\n",
    "\n",
    "\n",
    "    env = EnvironmentFactory.create(**env_config)\n",
    "    vecnormalize = load_vecnormalize(env_path, env)\n",
    "    vecnormalize.training = False\n",
    "    vecnormalize.norm_reward = False\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    performance = []\n",
    "    for k in range(n_comp):\n",
    "        print(\"Environment: \", target_task, \"component \", k)\n",
    "        if sorting == \"best_to_worst\":\n",
    "            components = pca.components_[:n_comp-k]\n",
    "        elif sorting == \"worst_to_best\":\n",
    "            components = pca.components_[k:]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown sorting: \", sorting)\n",
    "\n",
    "        performance_ep = []\n",
    "        for n in range(num_ep):\n",
    "            acts = []\n",
    "            cum_reward = 0\n",
    "            lstm_states = None\n",
    "            obs = env.reset()\n",
    "            episode_starts = np.ones((1,), dtype=bool)\n",
    "            done = False\n",
    "            timestep = 0\n",
    "            while not done: \n",
    "                timestep += 1\n",
    "                action, lstm_states = model.predict(vecnormalize.normalize_obs(obs),\n",
    "                                                        state=lstm_states,\n",
    "                                                        episode_start=episode_starts,\n",
    "                                                        deterministic=True,\n",
    "                                                        )\n",
    "                \n",
    "                action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "                action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "                obs, rewards, done, info = env.step(action_backproj.reshape(39,))\n",
    "                episode_starts = done\n",
    "                cum_reward += rewards\n",
    "            performance_ep.append(cum_reward)\n",
    "            print(f\"Episode {n}, reward: {cum_reward}\")\n",
    "        performance_ep = np.array(performance_ep) / 1000  # Transform the reward into the solved fraction\n",
    "        data_point = {'components':components,'solved_frac_mean': np.mean(performance_ep), 'solved_frac_sem': np.std(performance_ep) / np.sqrt(len(performance_ep))}\n",
    "        performance.append(data_point)\n",
    "        print(data_point)\n",
    "\n",
    "    fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_phase_1_{target_task}_{sorting}.pkl\")\n",
    "    fp_acts_pcs = open(fp, 'wb')\n",
    "    pickle.dump(performance, fp_acts_pcs)\n",
    "    fp_acts_pcs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the performance when projecting the actions on any possible subset of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: evaluate the model when projecting the actions on a subset of the PCs\n",
    "# best_to_worst removes low variance components first, worst to best remove the \n",
    "# high variance components first\n",
    "n_comp = 39\n",
    "num_ep = 100\n",
    "target_task_list = [\"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]  # \"early_baoding\",\n",
    "sorting = \"best_to_worst\"  # \"best_to_worst\", \"worst_to_best\"\n",
    "\n",
    "env_config = {\n",
    "        \"env_name\": \"MyoBaodingBallsP1\",\n",
    "        \"weighted_reward_keys\": {\n",
    "            \"pos_dist_1\": 0,\n",
    "            \"pos_dist_2\": 0,\n",
    "            \"act_reg\": 0,\n",
    "            \"solved\": 5,\n",
    "            \"done\": 0,\n",
    "            \"sparse\": 0,\n",
    "        },\n",
    "        # # \"goal_time_period\": [4, 6],  # phase 2: (4, 6)\n",
    "        # # \"goal_xrange\": (0.020, 0.030),  # phase 2: (0.020, 0.030)\n",
    "        # # \"goal_yrange\": (0.022, 0.032),  # phase 2: (0.022, 0.032)\n",
    "        # # # Randomization in physical properties of the baoding balls\n",
    "        \"obj_size_range\": (\n",
    "            0.020,\n",
    "            0.022,\n",
    "        ),  # (0.018, 0.024)   # Object size range. Nominal 0.022\n",
    "        \"obj_mass_range\": (\n",
    "            0.14,\n",
    "            0.16,\n",
    "            # # \"obj_friction_change\": (0.2, 0.001, 0.00002),  # (0.2, 0.001, 0.00002)\n",
    "        ),\n",
    "        \"task_choice\": \"fixed\",\n",
    "        \"seed\": 42\n",
    "    }\n",
    "env_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "model_path = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "for i in range(2, len(target_task_list) + 1):\n",
    "    target_task_combinations = list(combinations(target_task_list, i))\n",
    "    for target_task_sublist in target_task_combinations:\n",
    "        print(f\"Loading data for tasks {target_task_sublist}...\")\n",
    "        actions_list = []\n",
    "        for target_task in target_task_sublist:\n",
    "            # First choose the pca to use to project the actions\n",
    "            if target_task == \"baoding\":\n",
    "                actions = np.vstack(rollouts_df.action)\n",
    "            elif target_task == \"early_baoding\":\n",
    "                actions = np.vstack(early_baoding_rollouts_df.action)\n",
    "            else:\n",
    "                actions = np.vstack(df_dict[target_task].action)\n",
    "            actions_list.append(actions)\n",
    "        actions = np.vstack(actions_list)\n",
    "        \n",
    "        print(f\"Computing pca for tasks {target_task_sublist}...\")\n",
    "        pca = PCA(n_components=n_comp).fit(actions)\n",
    "        out_path = os.path.join(ROOT_DIR, \"data\", \"pca\", f\"pca_muscle_act_{'_'.join(target_task_sublist)}.joblib\")\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            joblib.dump(pca, f)\n",
    "        env = EnvironmentFactory.create(**env_config)\n",
    "        vecnormalize = load_vecnormalize(env_path, env)\n",
    "        vecnormalize.training = False\n",
    "        vecnormalize.norm_reward = False\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        print(f\"Running episodes for tasks {target_task_sublist}...\")\n",
    "        performance = []\n",
    "        for k in range(n_comp):\n",
    "            print(\"Environment: \", target_task_sublist, \"component \", k)\n",
    "            if sorting == \"best_to_worst\":\n",
    "                components = pca.components_[:n_comp-k]\n",
    "            elif sorting == \"worst_to_best\":\n",
    "                components = pca.components_[k:]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown sorting: \", sorting)\n",
    "\n",
    "            performance_ep = []\n",
    "            for n in range(num_ep):\n",
    "                acts = []\n",
    "                cum_reward = 0\n",
    "                lstm_states = None\n",
    "                obs = env.reset()\n",
    "                episode_starts = np.ones((1,), dtype=bool)\n",
    "                done = False\n",
    "                timestep = 0\n",
    "                while not done: \n",
    "                    timestep += 1\n",
    "                    action, lstm_states = model.predict(vecnormalize.normalize_obs(obs),\n",
    "                                                            state=lstm_states,\n",
    "                                                            episode_start=episode_starts,\n",
    "                                                            deterministic=True,\n",
    "                                                            )\n",
    "                    \n",
    "                    action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "                    action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "                    obs, rewards, done, info = env.step(action_backproj.reshape(39,))\n",
    "                    episode_starts = done\n",
    "                    cum_reward += rewards\n",
    "                performance_ep.append(cum_reward)\n",
    "                print(f\"Episode {n}, reward: {cum_reward}\")\n",
    "            performance_ep = np.array(performance_ep) / 1000  # Transform the reward into the solved fraction\n",
    "            data_point = {'components': components,'solved_frac_mean': np.mean(performance_ep), 'solved_frac_sem': np.std(performance_ep) / np.sqrt(len(performance_ep))}\n",
    "            performance.append(data_point)\n",
    "            print(data_point)\n",
    "\n",
    "        target_task_str = \"_\".join(target_task_sublist)\n",
    "        fp = os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_{target_task_str}_{sorting}.pkl\")\n",
    "        fp_acts_pcs = open(fp, 'wb')\n",
    "        pickle.dump(performance, fp_acts_pcs)\n",
    "        fp_acts_pcs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the performance curve for the different combinations of tasks\n",
    "def plot_explained_variance_ratio(exp_var, label, color, ax=None, fig=None):\n",
    "    if ax is None or fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.step(range(1, len(exp_var) + 1), exp_var, where='mid', linewidth=3, color=color, label=label)\n",
    "    ax.set_ylabel('Cum. explained variance',fontsize=16)\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    # plt.legend(fontsize=14,loc='best')\n",
    "    ax.tick_params(axis='both', labelsize=14)\n",
    "    # ax.axhline(y=0.95, color='black', linestyle='--', alpha=0.5)\n",
    "    # ax.axhline(y=0.85, color='black', linestyle='--', alpha=0.5)\n",
    "    # ax.text(0, 0.96, '95%', color = 'black', fontsize=18)\n",
    "    # ax.text(0, 0.86, '85%', color = 'black', fontsize=18)\n",
    "    return fig, ax\n",
    "\n",
    "def ev(X, X_approx, model_mean):\n",
    "    return 1 - np.sum((X - X_approx)**2) / np.sum((X - model_mean)**2)\n",
    "\n",
    "n_comp = 39\n",
    "sorting = \"best_to_worst\"\n",
    "target_task_list = [\"hand_pose\", \"hand_reach\", \"pen\", \"reorient\"]\n",
    "task_to_print_dict = {\n",
    "    \"baoding\": \"Baoding\",\n",
    "    \"early_baoding\": \"Baoding step 12\",\n",
    "    \"hand_pose\": \"Hand Pose\",\n",
    "    \"hand_reach\": \"Hand Reach\",\n",
    "    \"pen\": \"Pen\",\n",
    "    \"reorient\": \"Reorient\"\n",
    "}\n",
    "muscle_act = np.vstack(rollouts_df.muscle_act)\n",
    "for i in range(2, len(target_task_list) + 1):\n",
    "    target_task_combinations = list(combinations(target_task_list, i))\n",
    "    for target_task_sublist in target_task_combinations:\n",
    "        target_task_str = \"_\".join(target_task_sublist)\n",
    "        task_perf = pickle.load(open(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"performance_{target_task_str}_{sorting}.pkl\"), \"rb\"))\n",
    "        perfs_mean = np.array([d['solved_frac_mean'] for d in task_perf[::-1]])\n",
    "        perfs_sem = np.array([d['solved_frac_sem'] for d in task_perf[::-1]])\n",
    "        \n",
    "        pca = joblib.load(os.path.join(ROOT_DIR, \"data\", \"pca\", f\"pca_muscle_act_{target_task_str}.joblib\"))\n",
    "        \n",
    "        muscle_act_projected = pca.transform(muscle_act)\n",
    "        muscle_act_approx = pca.inverse_transform(muscle_act_projected)\n",
    "        exp_var = ev(muscle_act, muscle_act_approx, pca.mean_)\n",
    "        exp_var_ratio_list = [exp_var]\n",
    "        for i in range(1, n_comp):\n",
    "            muscle_act_projected[:, -i:] = 0\n",
    "            muscle_act_approx = pca.inverse_transform(muscle_act_projected)\n",
    "            exp_var = ev(muscle_act, muscle_act_approx, pca.mean_)\n",
    "            exp_var_ratio_list.append(exp_var)\n",
    "        exp_var_ratio_list.reverse()\n",
    "        \n",
    "        fig, ax1 = plt.subplots()\n",
    "        ax2 = ax1.twinx()\n",
    "        plot_explained_variance_ratio(exp_var_ratio_list, \"Explained variance\", 'black', ax=ax1, fig=fig)\n",
    "        ax2.errorbar(np.arange(1, n_comp + 1), perfs_mean, yerr=perfs_sem, fmt='-', color=\"dodgerblue\")\n",
    "        ax1.set_xlabel('Number of PCs', fontsize=16, labelpad=5)\n",
    "        ax2.set_ylabel('Solved fraction', fontsize=16, labelpad=5, color=\"dodgerblue\")\n",
    "        ax2.tick_params(axis='y', labelsize=14, colors=\"dodgerblue\")\n",
    "        ax1.set_title(\", \".join([task_to_print_dict[task] for task in target_task_sublist]), fontsize=16)\n",
    "        plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"performance_decay\", f\"perf_vs_comp_{target_task_str}.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "x = range(10)\n",
    "y1 = [i**2 for i in x]\n",
    "y2 = [i*10 for i in x]\n",
    "\n",
    "# Create the first plot\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot the first curve with the first y-axis\n",
    "ax1.plot(x, y1, color='blue')\n",
    "ax1.set_ylabel('Y1', color='blue')\n",
    "\n",
    "# Create a twin axes sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second curve with the second y-axis\n",
    "ax2.plot(x, y2, color='red')\n",
    "ax2.set_ylabel('Y2', color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Visualize the first principal actions\n",
    "1. Load the principal actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file from Basecamp : 'performance_actions_components_t'\n",
    "performance_components = pickle.load(open(os.path.join(ROOT_DIR, \"data\", \"basecamp\", \"performance_action_components_t.pkl\"),'rb'))\n",
    "principal_actions = [d['components'] for d in performance_components][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Save frames of each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_every = 15\n",
    "for action_idx in range(5):\n",
    "    env_name = \"CustomMyoBaodingBallsP2\"\n",
    "\n",
    "    config = set_config(period=5,rot_dir=\"cw\")\n",
    "    rollouts = []\n",
    "\n",
    "    eval_env = EnvironmentFactory.create(env_name, **config)\n",
    "    frames = []\n",
    "\n",
    "    for n in range(1): # Just one episode\n",
    "        eval_env.reset()\n",
    "        qpos = eval_env.init_qpos.copy()\n",
    "        qvel = eval_env.init_qvel.copy()\n",
    "        qpos[25] = 10\n",
    "        qpos[32] = 10\n",
    "        eval_env.sim.model.site_pos[eval_env.target1_sid, 2] = 10\n",
    "        eval_env.sim.model.site_pos[eval_env.target2_sid, 2] = 10\n",
    "        \n",
    "        eval_env.set_state(qpos, qvel)\n",
    "        timestep = 0\n",
    "        while timestep < 16 : \n",
    "            curr_frame = eval_env.render_camera_offscreen(['hand_top', 'hand_bottom', 'hand_side_inter', 'hand_side_exter', 'plam_lookat'])\n",
    "            frames.append(curr_frame)\n",
    "            timestep += 1\n",
    "            obs, rewards, done, info = eval_env.step(principal_actions[action_idx])\n",
    "\n",
    "    cam_frames = [l[1] for l in frames[::plot_every]]\n",
    "    num_frames = len(cam_frames)\n",
    "\n",
    "    print(\"Plotting component \", action_idx)\n",
    "    # Create a figure with a single row and the number of columns equal to the number of frames\n",
    "    fig, axes = plt.subplots(1, num_frames, figsize=(num_frames * 4, 4))\n",
    "\n",
    "    # Remove axes for all subplots\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Display each frame in its respective subplot\n",
    "    for i, frame in enumerate(cam_frames):\n",
    "        axes[i].imshow(frame[120:, 160:480])\n",
    "\n",
    "    plt.tight_layout()  # Adjust spacing between subplots\n",
    "    # plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"principal_component_{action_idx}_frames.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal components of the pose (rotation CCW)\n",
    "\n",
    "def plot_hand_pos(hand_pos, fig=None, ax=None):\n",
    "    env_name = \"CustomMyoBaodingBallsP2\"\n",
    "    env = EnvironmentFactory.create(env_name,)\n",
    "\n",
    "    env.sim.model.site_pos[env.target1_sid, 2] = 10\n",
    "    env.sim.model.site_pos[env.target2_sid, 2] = 10\n",
    "    qpos = np.concatenate((hand_pos, np.zeros(14)))\n",
    "    qpos[25] = 10\n",
    "    qpos[32] = 10\n",
    "    qvel = np.zeros(35)\n",
    "    env.set_state(qpos, qvel)\n",
    "    frame = env.render_camera_offscreen(['hand_bottom'])[0]\n",
    "\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.imshow(frame[120:, 160:480])\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "n_comp = 23\n",
    "hand_pos_mat = np.stack(rollouts_ccw.apply(lambda x: x.observation[:23], axis=1))\n",
    "pca = PCA(n_components=n_comp).fit(hand_pos_mat)\n",
    "\n",
    "# out_path = os.path.join(ROOT_DIR, \"data\", \"pca\", \"pca_pose_ccw.joblib\")\n",
    "# joblib.dump(pca, out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_path = os.path.join(ROOT_DIR, \"data\", \"pca\", \"pca_pose_ccw.joblib\")\n",
    "pca = joblib.load(pca_path)\n",
    "    \n",
    "num_frames = 2\n",
    "num_components = 5\n",
    "for i in range(num_components):  # Visualize the first principal components of the pose \n",
    "    fig, axes = plt.subplots(1, num_frames, figsize=(num_frames * 6, 6))\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "    plot_hand_pos(pca.mean_ + 1 * pca.components_[i], fig, axes[0])\n",
    "    plot_hand_pos(pca.mean_ - 1 * pca.components_[i], fig, axes[1])\n",
    "    fig.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"pose_principal_component_{i}_frames.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Heatmap of principal actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the principal actions (same data as that generated in point A.3.and loaded in point A.4.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file from Basecamp : 'performance_actions_components_t'\n",
    "performance_components = pickle.load(open(os.path.join(ROOT_DIR, \"data\", \"basecamp\", \"performance_action_components_t.pkl\"),'rb'))\n",
    "principal_actions = [d['components'] for d in performance_components][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.heatmap(pd.DataFrame(principal_actions[:13]),cmap=\"coolwarm\").get_figure()\n",
    "plt.xlabel('Muscles',fontsize=21)\n",
    "plt.ylabel('Principal actions',fontsize=21)\n",
    "plt.yticks(rotation=0,fontsize=17)\n",
    "plt.xticks(ticks=np.arange(1,40,3),labels=np.arange(1,40,3),rotation=45,fontsize=17)\n",
    "plt.subplots_adjust(left=0.15,bottom=0.2)\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", \"principal_components_muscle_weights.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0,199,21)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Principal action vs. phase of rotation (time step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. a. Compute the average principal actions \\\n",
    "b. Plot the PAs weights vs. time for the rotation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_muscles = 39\n",
    "num_components_plot = 10\n",
    "t_min = 1; t_max = 200  # Remove the first step\n",
    "\n",
    "pca = PCA(n_components=num_muscles)\n",
    "mean_actions = np.vstack(rollouts_cw.groupby(\"step\")[\"action\"].mean())\n",
    "mean_weights = pca.fit_transform(mean_actions)\n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "weights=[]\n",
    "for j in range(num_components_plot):\n",
    "    norm_weights = minmax.fit_transform(mean_weights[t_min:,j].reshape(t_max-t_min,1))\n",
    "    # norm_weights = minmax.fit_transform(mean_weights[:, j].reshape(200, 1))\n",
    "    weights.append(norm_weights)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 2.8)) \n",
    "sns.heatmap(pd.DataFrame(np.squeeze(weights)), cmap=\"coolwarm\", ax=ax)\n",
    "# sns.heatmap(mean_weights.T,cmap=\"coolwarm\", ax=ax)\n",
    "plt.yticks(ticks=np.arange(1,num_components_plot + 1,1)-0.5,labels=np.arange(1,num_components_plot + 1,1),rotation=0,fontsize=16)\n",
    "plt.xticks(rotation=45,ticks=np.linspace(t_min-1, t_max-1, 6),labels=np.linspace(t_min//40, t_max//40, 6),fontsize=16)\n",
    "plt.xlabel('Time [s]',fontsize=21)\n",
    "plt.ylabel('Principal\\nactions',fontsize=21)\n",
    "plt.subplots_adjust(left=0.15,bottom=0.2)\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"top_{num_components_plot}_principal_components_episode.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_joints = 23\n",
    "num_components_plot = 10\n",
    "t_min = 1; t_max = 200 # Remove the first step\n",
    "\n",
    "pca = PCA(n_components=num_joints)\n",
    "mean_poses = np.vstack(rollouts_cw.groupby(\"step\")[\"observation\"].mean())[:, :23]\n",
    "mean_weights = pca.fit_transform(mean_poses)\n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "weights=[]\n",
    "for j in range(num_components_plot):\n",
    "    norm_weights = minmax.fit_transform(mean_weights[t_min:,j].reshape(t_max-t_min,1))\n",
    "    weights.append(norm_weights)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 2.8)) \n",
    "sns.heatmap(pd.DataFrame(np.squeeze(weights)), cmap=\"coolwarm\", ax=ax)\n",
    "# sns.heatmap(mean_weights.T,cmap=\"coolwarm\", ax=ax)\n",
    "plt.yticks(ticks=np.arange(1,num_components_plot + 1,1)-0.5,labels=np.arange(1,num_components_plot + 1,1),rotation=0,fontsize=16)\n",
    "plt.xticks(rotation=45,ticks=np.linspace(t_min-1, t_max-1, 6),labels=np.linspace(t_min//40, t_max//40, 6),fontsize=16)\n",
    "plt.xlabel('Time [s]',fontsize=21)\n",
    "plt.ylabel('Principal\\nposes',fontsize=21)\n",
    "plt.subplots_adjust(left=0.15,bottom=0.2)\n",
    "# plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"top_{num_components_plot}_principal_poses_episode.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_num = 100\n",
    "obs_list = rollouts_df[(rollouts_df.episode == episode_num) & (rollouts_df.task == \"cw\")].observation.to_list()\n",
    "ball1_pos = np.vstack([o[23:26] for o in obs_list]) * 100\n",
    "ball2_pos = np.vstack([o[29:32] for o in obs_list]) * 100\n",
    "\n",
    "target1_pos = np.vstack([o[35:38] for o in obs_list]) * 100\n",
    "target2_pos = np.vstack([o[38:41] for o in obs_list]) * 100\n",
    "\n",
    "target1_center = np.mean(target1_pos, axis=0)\n",
    "target2_center = np.mean(target2_pos, axis=0)\n",
    "\n",
    "ball1_rel_pos = ball1_pos - target1_center\n",
    "ball2_rel_pos = ball2_pos - target2_center\n",
    "\n",
    "ball1_angle = np.arctan2(ball1_rel_pos[:, 1], ball1_rel_pos[:, 0]) / np.pi * 180\n",
    "ball2_angle = np.arctan2(ball2_rel_pos[:, 1], ball2_rel_pos[:, 0]) / np.pi * 180\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 1.5))\n",
    "\n",
    "cmap = get_cmap(\"coolwarm\")\n",
    "\n",
    "ax.plot(ball1_rel_pos[:, 0], label=\"x ball 1\", color=cmap(0.))\n",
    "ax.plot(ball1_rel_pos[:, 1], label=\"y ball 1\", color=cmap(0.15))\n",
    "ax.plot(ball1_rel_pos[:, 2], label=\"z ball 1\", color=cmap(0.3))\n",
    "\n",
    "ax.plot(ball2_rel_pos[:, 0], label=\"x ball 2\", color=cmap(0.99))\n",
    "ax.plot(ball2_rel_pos[:, 1], label=\"y ball 2\", color=cmap(0.85))\n",
    "ax.plot(ball2_rel_pos[:, 2], label=\"z ball 2\", color=cmap(0.7))\n",
    "\n",
    "ax.xaxis.set_ticks([])\n",
    "ax.set_ylabel(\"Position [cm]\")\n",
    "ax.legend(bbox_to_anchor=(1.15, 1.11))\n",
    "# plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"ball_coords_episode_{episode_num}.png\"), format=\"png\", dpi=1000, bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series of the joint positions\n",
    "episode_num = 100\n",
    "obs_list = rollouts_df[(rollouts_df.episode == episode_num) & (rollouts_df.task == \"cw\")].observation.to_list()\n",
    "joint_pos = np.vstack([o[:23] for o in obs_list])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 1.5))\n",
    "\n",
    "cmap = get_cmap(\"plasma\")\n",
    "\n",
    "for joint_idx, joint_pose in enumerate(joint_pos.T):\n",
    "    color = cmap((joint_idx + 1) / (joint_pos.shape[1] + 2))\n",
    "    ax.plot(joint_pose, color=color, alpha=0.5)\n",
    "\n",
    "# ax.plot(ball1_rel_pos[:, 0], label=\"x ball 1\", color=cmap(0.))\n",
    "# ax.plot(ball1_rel_pos[:, 1], label=\"y ball 1\", color=cmap(0.15))\n",
    "# ax.plot(ball1_rel_pos[:, 2], label=\"z ball 1\", color=cmap(0.3))\n",
    "\n",
    "# ax.plot(ball2_rel_pos[:, 0], label=\"x ball 2\", color=cmap(0.99))\n",
    "# ax.plot(ball2_rel_pos[:, 1], label=\"y ball 2\", color=cmap(0.85))\n",
    "# ax.plot(ball2_rel_pos[:, 2], label=\"z ball 2\", color=cmap(0.7))\n",
    "\n",
    "\n",
    "ax.xaxis.set_ticks([])\n",
    "ax.set_ylabel(\"Angle [rad]\")\n",
    "# ax.legend(bbox_to_anchor=(1.15, 1.11))\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"joint_pos_episode_{episode_num}.png\"), format=\"png\", dpi=1000, bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series of the muscle activations\n",
    "episode_num = 100\n",
    "muscle_act = np.vstack(rollouts_df[(rollouts_df.episode == episode_num) & (rollouts_df.task == \"cw\")].muscle_act)\n",
    "muscle_act = muscle_act\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 1.5))\n",
    "\n",
    "cmap = get_cmap(\"plasma\")\n",
    "\n",
    "for muscle_idx, act in enumerate(muscle_act.T):\n",
    "    color = cmap((muscle_idx + 1) / (muscle_act.shape[1] + 2))\n",
    "    ax.plot(act, color=color, alpha=0.3)\n",
    "\n",
    "ax.xaxis.set_ticks([])\n",
    "ax.set_ylabel(\"Muslce activation\")\n",
    "ax.set_yticks([0, 0.5, 1])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "# ax.legend(bbox_to_anchor=(1.15, 1.11))\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"muscle_act_episode_{episode_num}.png\"), format=\"png\", dpi=1000, bbox_inches=\"tight\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.vstack(rollouts_df.muscle_act))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save screenshot at different steps of the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ep = 1\n",
    "n_comp = 39\n",
    "\n",
    "PATH_TO_NORMALIZED_ENV = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "PATH_TO_PRETRAINED_NET = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "env_name = \"CustomMyoBaodingBallsP2\"\n",
    "render = False\n",
    "\n",
    "config = set_config(period=5,rot_dir=\"cw\")\n",
    "rollouts = []\n",
    "\n",
    "envs = make_parallel_envs(env_name, config, num_env=1)\n",
    "envs = VecNormalize.load(PATH_TO_NORMALIZED_ENV, envs)\n",
    "envs.training = False\n",
    "envs.norm_reward = False\n",
    "custom_objects = {\n",
    "    \"learning_rate\": lambda _: 0,\n",
    "    \"lr_schedule\": lambda _: 0,\n",
    "    \"clip_range\": lambda _: 0,\n",
    "}\n",
    "model = RecurrentPPO.load(\n",
    "        PATH_TO_PRETRAINED_NET, env=envs, device=\"cpu\", custom_objects=custom_objects\n",
    "    )\n",
    "\n",
    "eval_model = model\n",
    "eval_env = EnvironmentFactory.create(env_name,**config)\n",
    "frames = []\n",
    "for n in range(num_ep):\n",
    "    print(n)\n",
    "    cum_reward = 0\n",
    "    lstm_states = None\n",
    "    obs = eval_env.reset()\n",
    "    episode_starts = np.ones((1,), dtype=bool)\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    while not done: \n",
    "        curr_frame = eval_env.render_camera_offscreen(['hand_top', 'hand_bottom', 'hand_side_inter', 'hand_side_exter', 'plam_lookat'])\n",
    "        frames.append(curr_frame)            \n",
    "        timestep += 1\n",
    "        action, lstm_states = eval_model.predict(envs.normalize_obs(obs),\n",
    "                                                state=lstm_states,\n",
    "                                                episode_start=episode_starts,\n",
    "                                                deterministic=True,\n",
    "                                                )\n",
    "                                                    \n",
    "        obs, rewards, done, info = eval_env.step(action)\n",
    "        episode_starts = done\n",
    "        cum_reward += rewards   \n",
    "    print(cum_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_frames = [l[1] for l in frames[20::30]]\n",
    "num_frames = len(cam_frames)\n",
    "\n",
    "# Create a figure with a single row and the number of columns equal to the number of frames\n",
    "fig, axes = plt.subplots(1, num_frames, figsize=(num_frames * 8, 8))\n",
    "\n",
    "# Remove axes for all subplots\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Display each frame in its respective subplot\n",
    "for i, frame in enumerate(cam_frames):\n",
    "    axes[i].imshow(frame)\n",
    "\n",
    "plt.tight_layout()  # Adjust spacing between subplots\n",
    "plt.savefig(os.path.join(ROOT_DIR, \"data\", \"figures\", \"panel_3\", f\"episode_frames.png\"), format=\"png\", dpi=600, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyoChallenge2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
