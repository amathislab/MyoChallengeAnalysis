{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyoChallenge Policy Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'set_config' from 'functions' (/home/ingster/Bureau/MyoChallengeAnalysis/src/functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m make_parallel_envs,set_config\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvec_env\u001b[39;00m \u001b[39mimport\u001b[39;00m VecNormalize\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'set_config' from 'functions' (/home/ingster/Bureau/MyoChallengeAnalysis/src/functions.py)"
     ]
    }
   ],
   "source": [
    "from definitions import ROOT_DIR\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from functions import make_parallel_envs,set_config\n",
    "import pickle\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from envs.environment_factory import EnvironmentFactory\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analysis of principal actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Performance vs. number of dimensions removed in the action space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ep = 20\n",
    "n_comp = 39\n",
    "\n",
    "PATH_TO_NORMALIZED_ENV = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "PATH_TO_PRETRAINED_NET = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "env_name = \"CustomMyoBaodingBallsP2\"\n",
    "render = True\n",
    "\n",
    "config = set_config(period=5,rot_dir=\"cw\")\n",
    "rollouts = []\n",
    "\n",
    "envs = make_parallel_envs(env_name, config, num_env=1)\n",
    "envs = VecNormalize.load(PATH_TO_NORMALIZED_ENV, envs)\n",
    "envs.training = False\n",
    "envs.norm_reward = False\n",
    "custom_objects = {\n",
    "    \"learning_rate\": lambda _: 0,\n",
    "    \"lr_schedule\": lambda _: 0,\n",
    "    \"clip_range\": lambda _: 0,\n",
    "}\n",
    "model = RecurrentPPO.load(\n",
    "        PATH_TO_PRETRAINED_NET, env=envs, device=\"cpu\", custom_objects=custom_objects\n",
    "    )\n",
    "\n",
    "eval_model = model\n",
    "eval_env = EnvironmentFactory.create(env_name,**config)\n",
    "\n",
    "for n in range(num_ep):\n",
    "    acts = []\n",
    "    cum_reward = 0\n",
    "    lstm_states = None\n",
    "    obs = eval_env.reset()\n",
    "    print(eval_env.which_task)\n",
    "    episode_starts = np.ones((1,), dtype=bool)\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    while not done: \n",
    "        if render :\n",
    "            eval_env.sim.render(mode=\"window\")\n",
    "            \n",
    "        timestep += 1\n",
    "        action, lstm_states = eval_model.predict(envs.normalize_obs(obs),\n",
    "                                                state=lstm_states,\n",
    "                                                episode_start=episode_starts,\n",
    "                                                deterministic=True,\n",
    "                                                )\n",
    "                                                    \n",
    "        obs, rewards, done, info = eval_env.step(action)\n",
    "        episode_starts = done\n",
    "        cum_reward += rewards\n",
    "        acts.append(action)\n",
    "    print('episode %s : '%n,cum_reward)\n",
    "    rollouts.append({'reward':cum_reward,'action':np.array(acts)})\n",
    "\n",
    "your_path = \"\"\n",
    "fp_rollouts = open(your_path, 'wb')\n",
    "pickle.dump(rollouts,fp_rollouts)\n",
    "fp_rollouts.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABI TO INDICATE\n",
    "your_path = \"\"\n",
    "rollouts = pickle.load(open(your_path,'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. a. Compute the principal actions\\\n",
    "b. Compute the performance when the actions are projected on a progressively lower-dimensional action subspace\\\n",
    "c. Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.concatenate([rollout['action'] for rollout in rollouts])\n",
    "performance = []\n",
    "pca = PCA(n_components=n_comp).fit(actions)\n",
    "\n",
    "for k in range(n_comp):\n",
    "    print(k)\n",
    "    components = pca.components_[:n_comp-k]\n",
    "    performance_ep = []\n",
    "    for n in range(num_ep):\n",
    "        acts = []\n",
    "        cum_reward = 0\n",
    "        lstm_states = None\n",
    "        obs = eval_env.reset()\n",
    "        episode_starts = np.ones((1,), dtype=bool)\n",
    "        done = False\n",
    "        timestep = 0\n",
    "        while not done: \n",
    "            if render :\n",
    "                eval_env.sim.render(mode=\"window\")\n",
    "                \n",
    "            timestep += 1\n",
    "            action, lstm_states = eval_model.predict(envs.normalize_obs(obs),\n",
    "                                                    state=lstm_states,\n",
    "                                                    episode_start=episode_starts,\n",
    "                                                    deterministic=True,\n",
    "                                                    )\n",
    "            \n",
    "            action_proj = np.dot(action.reshape(-1,39)-pca.mean_,components.T)\n",
    "            action_backproj = np.dot(action_proj,components)+pca.mean_\n",
    "            obs, rewards, done, info = eval_env.step(action_backproj.reshape(39,))\n",
    "            episode_starts = done\n",
    "            cum_reward += rewards\n",
    "        performance_ep.append(cum_reward)\n",
    "    performance.append({'components':components,'reward':np.mean(np.array(performance_ep))})\n",
    "\n",
    "path = \"\"\n",
    "fp_acts_pcs = open(path, 'wb')\n",
    "pickle.dump(performance,fp_acts_pcs)\n",
    "fp_acts_pcs.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABI TO INDICATE\n",
    "your_path = \"\"\n",
    "performance_components = pickle.load(open(your_path,'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the performance vs. number of dimensions removed in the action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs = [d['reward'] for d in performance_components]\n",
    "comps = [d['components'] for d in performance_components]\n",
    "plt.plot([k for k in range(n_comp)],perfs,linewidth=1)\n",
    "plt.xlabel('Number of dimensions \\nremoved in the action space',fontsize=21,labelpad=10)\n",
    "plt.ylabel('Cumulative reward',fontsize=21,labelpad=10)\n",
    "plt.yticks(fontsize=21)\n",
    "plt.xticks(fontsize=21)\n",
    "plt.subplots_adjust(left=0.2,bottom=0.23)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Heatmap of principal actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the principal actions (data generated in point a.3.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABI TO INDICATE\n",
    "fp = \"\"\n",
    "performance_components = pickle.load(open(fp,'rb'))\n",
    "principal_actions = [d['components'] for d in performance_components][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.heatmap(pd.DataFrame(principal_actions[:13]),cmap=\"coolwarm\").get_figure()\n",
    "plt.xlabel('Action dimensions',fontsize=21)\n",
    "plt.ylabel('Principal actions',fontsize=21)\n",
    "plt.yticks(rotation=0,fontsize=17)\n",
    "plt.xticks(ticks=np.arange(1,40,3),labels=np.arange(1,40,3),rotation=45,fontsize=17)\n",
    "plt.subplots_adjust(left=0.15,bottom=0.2)\n",
    "plt.plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Principal action vs. phase of rotation (time step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate and save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ep = 100\n",
    "n_comp = 39\n",
    "\n",
    "PATH_TO_NORMALIZED_ENV = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/env.pkl\",\n",
    ")\n",
    "PATH_TO_PRETRAINED_NET = os.path.join(\n",
    "    ROOT_DIR,\n",
    "    \"trained_models/curriculum_steps_complete_baoding_winner/32_phase_2_smaller_rate_resume/model.zip\",\n",
    ")\n",
    "\n",
    "env_name = \"CustomMyoBaodingBallsP2\"\n",
    "render = False\n",
    "\n",
    "config = set_config(period=5,rot_dir=\"cw\")\n",
    "rollouts = []\n",
    "\n",
    "envs = make_parallel_envs(env_name, config, num_env=1)\n",
    "envs = VecNormalize.load(PATH_TO_NORMALIZED_ENV, envs)\n",
    "envs.training = False\n",
    "envs.norm_reward = False\n",
    "custom_objects = {\n",
    "    \"learning_rate\": lambda _: 0,\n",
    "    \"lr_schedule\": lambda _: 0,\n",
    "    \"clip_range\": lambda _: 0,\n",
    "}\n",
    "model = RecurrentPPO.load(\n",
    "        PATH_TO_PRETRAINED_NET, env=envs, device=\"cpu\", custom_objects=custom_objects\n",
    "    )\n",
    "\n",
    "eval_model = model\n",
    "eval_env = EnvironmentFactory.create(env_name,**config)\n",
    "actions = []\n",
    "for n in range(num_ep):\n",
    "    print(n)\n",
    "    acts_1ep = []\n",
    "    cum_reward = 0\n",
    "    lstm_states = None\n",
    "    obs = eval_env.reset()\n",
    "    episode_starts = np.ones((1,), dtype=bool)\n",
    "    done = False\n",
    "    timestep = 0\n",
    "    while not done: \n",
    "        if render :\n",
    "            eval_env.sim.render(mode=\"window\")\n",
    "            \n",
    "        timestep += 1\n",
    "        action, lstm_states = eval_model.predict(envs.normalize_obs(obs),\n",
    "                                                state=lstm_states,\n",
    "                                                episode_start=episode_starts,\n",
    "                                                deterministic=True,\n",
    "                                                )\n",
    "                                                    \n",
    "        obs, rewards, done, info = eval_env.step(action)\n",
    "        episode_starts = done\n",
    "        cum_reward += rewards   \n",
    "        acts_1ep.append(action)\n",
    "    if len(acts_1ep) < 200 :\n",
    "        temp = np.zeros((200,39))\n",
    "        temp[:len(acts_1ep)] += acts_1ep\n",
    "        acts_1ep = temp\n",
    "    actions.append(np.array(acts_1ep))\n",
    "\n",
    "your_path = \"\"\n",
    "fp_rollouts = open(your_path, 'wb')\n",
    "pickle.dump(actions,fp_rollouts)\n",
    "fp_rollouts.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABI INSERT PATH\n",
    "fp = '/home/ingster/Bureau/SIL-BigResults/rollout_100ep'\n",
    "actions = pickle.load(open(fp,'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. a. Compute the average principal actions \\\n",
    "b. Plot the PAs weights vs. time for the rotation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_comp)\n",
    "mean_actions = sum(actions)/len(actions)\n",
    "mean_weights = pca.fit_transform(mean_actions)\n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "weights=[]\n",
    "t_min = 13; t_max = 200 # Rotation phase (transient phase from 0 to 13 time steps)\n",
    "for j in range(15):\n",
    "    norm_weights = minmax.fit_transform(mean_weights[t_min:,j].reshape(t_max-t_min,1))\n",
    "    weights.append(norm_weights)\n",
    "\n",
    "fig = sns.heatmap(pd.DataFrame(np.squeeze(weights)),cmap=\"coolwarm\").get_figure()\n",
    "plt.yticks(ticks=np.arange(1,16,1),labels=np.arange(1,16,1),rotation=0,fontsize=17)\n",
    "plt.xticks(rotation=45,ticks=np.arange(0,t_max-t_min,21),labels=np.arange(t_min,t_max,21),fontsize=16)\n",
    "plt.xlabel('Time step',fontsize=21)\n",
    "plt.ylabel('Principal actions',fontsize=21)\n",
    "plt.subplots_adjust(left=0.15,bottom=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis of principal actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyoChallenge2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
